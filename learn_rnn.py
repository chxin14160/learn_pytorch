import torch
from torch import nn
import common
import collections # æä¾›é«˜æ€§èƒ½çš„å®¹å™¨æ•°æ®ç±»å‹ï¼Œæ›¿ä»£Pythonçš„é€šç”¨å®¹å™¨(å¦‚ dict, list, set, tuple)
import re # ä¾›æ­£åˆ™è¡¨è¾¾å¼æ”¯æŒï¼Œç”¨äºå­—ç¬¦ä¸²åŒ¹é…ã€æœç´¢å’Œæ›¿æ¢
import random
from torch.nn import functional as F


def learn_SequenceModel():
    # 1. æ•°æ®ç”ŸæˆåŠå¯è§†åŒ–
    # ç”Ÿæˆå«å™ªå£°çš„å‘¨æœŸæ€§æ—¶é—´åºåˆ—æ•°æ®ï¼ˆæ­£å¼¦æ³¢+å™ªå£°ï¼‰
    T = 1000  # æ€»å…±äº§ç”Ÿ1000ä¸ªç‚¹
    time = torch.arange(1, T + 1, dtype=torch.float32)      # æ—¶é—´æ­¥ [1, 2, ..., 1000]
    # (T,) æ˜¯è¡¨ç¤ºå¼ é‡å½¢çŠ¶ï¼ˆshapeï¼‰çš„å…ƒç»„ï¼Œç”¨äºæŒ‡å®šç”Ÿæˆçš„é«˜æ–¯å™ªå£°(æ­£æ€åˆ†å¸ƒ)çš„ç»´åº¦ï¼ˆæŒ‡å®šç”Ÿæˆä¸€ç»´å¼ é‡ï¼Œé•¿åº¦ä¸ºTï¼‰
    x = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,)) # ç”Ÿæˆæ­£å¼¦ä¿¡å· + é«˜æ–¯å™ªå£°
    print(f"xçš„å½¢çŠ¶ï¼š{x.shape}")
    common.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3)) # ç»˜åˆ¶æ—¶é—´åºåˆ—

    # 2. æ„é€ ç‰¹å¾ä¸æ ‡ç­¾
    # å°†æ—¶é—´åºåˆ—è½¬æ¢ä¸ºç›‘ç£å­¦ä¹ é—®é¢˜ï¼ˆç”¨å‰4ä¸ªç‚¹é¢„æµ‹ç¬¬5ä¸ªç‚¹
    tau = 4 # ç”¨è¿‡å»4ä¸ªæ—¶é—´æ­¥é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥
    features = torch.zeros((T - tau, tau)) # ç‰¹å¾çŸ©é˜µå½¢çŠ¶: (996, 4)ï¼ˆæ€»å…±996ä¸ªæœ‰æ•ˆæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬å¯¹åº”4ä¸ªç‰¹å¾ï¼‰
    for i in range(tau):
        features[:, i] = x[i: T - tau + i] # æ»‘åŠ¨çª—å£å¡«å……ç‰¹å¾
    labels = x[tau:].reshape((-1, 1))      # æ ‡ç­¾å½¢çŠ¶: (996, 1) ï¼ˆå‰4é¡¹ä¸¢å¼ƒï¼‰

    # 3. æ•°æ®åŠ è½½å™¨
    # åˆ›å»ºæ•°æ®è¿­ä»£å™¨ï¼Œæ”¯æŒæ‰¹é‡è®­ç»ƒ
    batch_size, n_train = 16, 600 # æ‰¹é‡å¤§å°16ï¼Œè®­ç»ƒé›†600æ ·æœ¬
    # å°†å‰n_trainä¸ªæ ·æœ¬ç”¨äºè®­ç»ƒ
    train_iter = common.load_array((features[:n_train], labels[:n_train]),
                                batch_size, is_train=True) # åˆ›å»ºæ•°æ®è¿­ä»£å™¨ï¼Œæ”¯æŒæ‰¹é‡è®­ç»ƒ

    # 4. ç½‘ç»œåˆå§‹åŒ–
    # åˆå§‹åŒ–ç½‘ç»œæƒé‡çš„å‡½æ•°
    def init_weights(m):
        if type(m) == nn.Linear:
            nn.init.xavier_uniform_(m.weight) # Xavieråˆå§‹åŒ–æƒé‡

    # å®šä¹‰ä¸€ä¸ªç®€å•çš„å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰
    def get_net():
        net = nn.Sequential(nn.Linear(4, 10),  # è¾“å…¥å±‚(4) â†’ éšè—å±‚(10)
                            nn.ReLU(),         # æ¿€æ´»å‡½æ•°
                            nn.Linear(10, 1))  # éšè—å±‚(10) â†’ è¾“å‡ºå±‚(1)
        net.apply(init_weights)  # åº”ç”¨åˆå§‹åŒ–
        return net

    # å¹³æ–¹æŸå¤±ã€‚æ³¨æ„ï¼šMSELossè®¡ç®—å¹³æ–¹è¯¯å·®æ—¶ä¸å¸¦ç³»æ•°1/2
    # reduction='none' è¿”å›æ¯ä¸ªæ ·æœ¬çš„æŸå¤±ï¼Œåç»­éœ€æ‰‹åŠ¨ .sum() æˆ– .mean()
    loss = nn.MSELoss(reduction='none')  # å‡æ–¹è¯¯å·®æŸå¤±ï¼Œä¸è‡ªåŠ¨æ±‚å’Œ/å¹³å‡

    # å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•
    def evaluate_loss(net, data_iter, loss):  #@save
        """è¯„ä¼°ç»™å®šæ•°æ®é›†ä¸Šæ¨¡å‹çš„æŸå¤±"""
        metric = common.Accumulator(2)  # æŸå¤±çš„æ€»å’Œ,æ ·æœ¬æ•°é‡
        for X, y in data_iter:
            out = net(X)             # æ¨¡å‹é¢„æµ‹è¾“å‡ºç»“æœ
            y = y.reshape(out.shape) # å°†å®é™…æ ‡ç­¾yçš„å½¢çŠ¶è°ƒæ•´ä¸ºä¸æ¨¡å‹è¾“å‡ºoutä¸€è‡´
            l = loss(out, y)         # æ¨¡å‹è¾“å‡ºoutä¸å®é™…æ ‡ç­¾yä¹‹é—´çš„æŸå¤±
            metric.add(l.sum(), l.numel()) # å°†æŸå¤±æ€»å’Œ å’Œ æ ·æœ¬æ€»æ•° ç´¯åŠ åˆ°metricä¸­
        return metric[0] / metric[1] # æŸå¤±æ€»å’Œ/é¢„æµ‹æ€»æ•°ï¼Œå³å¹³å‡æŸå¤±

    def train(net, train_iter, loss, epochs, lr):
        trainer = torch.optim.Adam(net.parameters(), lr)  # Adamä¼˜åŒ–å™¨
        for epoch in range(epochs):
            for X, y in train_iter:
                trainer.zero_grad()       # æ¢¯åº¦æ¸…é›¶
                l = loss(net(X), y)       # è®¡ç®—æŸå¤±ï¼ˆå½¢çŠ¶[batch_size, 1]ï¼‰
                l.sum().backward()        # åå‘ä¼ æ’­ï¼ˆå¯¹æ‰€æœ‰æ ·æœ¬æŸå¤±æ±‚å’Œï¼‰
                trainer.step()            # æ›´æ–°å‚æ•°
            # æ‰“å°è®­ç»ƒæŸå¤±ï¼ˆå‡è®¾evaluate_lossæ˜¯è‡ªå®šä¹‰å‡½æ•°ï¼‰
            print(f'epoch {epoch + 1}, '
                  f'loss: {evaluate_loss(net, train_iter, loss):f}')

    net = get_net()      # åˆå§‹åŒ–ç½‘ç»œ
    train(net, train_iter, loss, 5, 0.01)  # è®­ç»ƒ5ä¸ªepochï¼Œå­¦ä¹ ç‡0.01


    # å•æ­¥é¢„æµ‹ï¼šæ¨¡å‹é¢„æµ‹ä¸‹ä¸€æ—¶é—´æ­¥çš„èƒ½åŠ›
    onestep_preds = net(features)
    common.plot([time, time[tau:]],
             [x.detach().numpy(), onestep_preds.detach().numpy()], 'time',
             'x', legend=['data', '1-step preds'], xlim=[1, 1000],
             figsize=(6, 3))

    # ç®€å•çš„Kæ­¥é¢„æµ‹ï¼šä½¿ç”¨é¢„æµ‹ æ¥è¿›è¡ŒKæ­¥é¢„æµ‹ï¼ˆé€’å½’é¢„æµ‹ï¼‰
    # æ˜¯ä¸¥æ ¼çš„é€’å½’é¢„æµ‹ï¼Œæ¯ä¸ªæ–°é¢„æµ‹éƒ½åŸºäºä¹‹å‰çš„é¢„æµ‹
    # æ½œåœ¨é—®é¢˜ï¼šé€’å½’é¢„æµ‹çš„è¯¯å·®ä¼šç´¯ç§¯ï¼Œå› ä¸ºæ¯ä¸ªé¢„æµ‹éƒ½åŸºäºä¹‹å‰çš„é¢„æµ‹
    multistep_preds = torch.zeros(T) # åˆå§‹åŒ–é¢„æµ‹ç»“æœå¼ é‡
    multistep_preds[: n_train + tau] = x[: n_train + tau] # ç”¨çœŸå®å€¼å¡«å……å‰é¢å·²çŸ¥çš„çœŸå®å€¼
    for i in range(n_train + tau, T): # é€’å½’é¢„æµ‹
        # ä½¿ç”¨å‰tauä¸ªé¢„æµ‹å€¼ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå€¼
        multistep_preds[i] = net(
            multistep_preds[i - tau:i].reshape((1, -1)))

    common.plot([time, time[tau:], time[n_train + tau:]],
             [x.detach().numpy(), onestep_preds.detach().numpy(),
              multistep_preds[n_train + tau:].detach().numpy()], 'time',
             'x', legend=['data', '1-step preds', 'multistep preds'],
             xlim=[1, 1000], figsize=(6, 3))


    # å¤šæ­¥é¢„æµ‹ï¼ˆåºåˆ—é¢„æµ‹ï¼‰
    # æ˜¯åºåˆ—é¢„æµ‹ï¼Œå¯ä»¥åŒæ—¶è·å¾—å¤šä¸ªæœªæ¥æ—¶é—´æ­¥çš„é¢„æµ‹ï¼ˆè™½ç„¶è¿™äº›ä¸­é—´é¢„æµ‹ä¹ŸåŸºäºä¹‹å‰çš„é¢„æµ‹ï¼‰
    # æ½œåœ¨é—®é¢˜ï¼šè™½ç„¶èƒ½ä¸€æ¬¡é¢„æµ‹å¤šä¸ªæ­¥é•¿ï¼Œä½†é•¿æœŸé¢„æµ‹ä»ç„¶ä¾èµ–ä¸­é—´é¢„æµ‹ç»“æœ
    max_steps = 64 # æœ€å¤§é¢„æµ‹æ­¥æ•°

    # åˆå§‹åŒ–ç‰¹å¾å¼ é‡ï¼Œ(è¦é¢„æµ‹çš„æ ·æœ¬æ•°,ç‰¹å¾æ•°),å…¶ä¸­
    # å‰ tau åˆ—ï¼šå­˜å‚¨çœŸå®å†å²æ•°æ®ï¼ˆä½œä¸ºè¾“å…¥ï¼‰
    # å max_steps åˆ—ï¼šå­˜å‚¨æ¨¡å‹é¢„æµ‹çš„æœªæ¥å€¼
    # T-tau-max_steps+1æ˜¯å¯è®¡ç®—çš„æ—¶é—´çª—å£æ•°é‡ï¼Œç‰¹å¾æ•°(tauåˆ—çœŸå®æ•°æ® + max_stepsåˆ—é¢„æµ‹æ•°æ®)
    features = torch.zeros((T - tau - max_steps + 1, tau + max_steps))

    # å‰tauåˆ—ç”¨çœŸå®å€¼å¡«å……
    # åˆ—iï¼ˆi<tauï¼‰æ˜¯æ¥è‡ªxçš„è§‚æµ‹(å®é™…çœŸå®å€¼)ï¼Œå…¶æ—¶é—´æ­¥ä»ï¼ˆiï¼‰åˆ°ï¼ˆi+T-tau-max_steps+1ï¼‰
    print(f"çœŸå®å€¼å¡«å……ï¼š{x[i: i + T - tau - max_steps + 1].shape}") # torch.Size([1])
    for i in range(tau):
        features[:, i] = x[i: i + T - tau - max_steps + 1]
        # å¯¹äº i=0ï¼Œfeatures[:, 0] = x[0 : 0 + 927]ï¼ˆå³ x[0] åˆ° x[926]ï¼‰
        # å¯¹äº i=1ï¼Œfeatures[:, 1] = x[1 : 1 + 927]ï¼ˆå³ x[1] åˆ° x[927]ï¼‰
        # ...
        # å¯¹äº i=9ï¼Œfeatures[:, 9] = x[9 : 9 + 927]ï¼ˆå³ x[9] åˆ° x[935]ï¼‰

    # åmax_stepsåˆ—ç”¨æ¨¡å‹é¢„æµ‹å¡«å……
    # åˆ—iï¼ˆi>=tauï¼‰æ˜¯æ¥è‡ªï¼ˆi-tau+1ï¼‰æ­¥çš„é¢„æµ‹ï¼Œå…¶æ—¶é—´æ­¥ä»ï¼ˆiï¼‰åˆ°ï¼ˆi+T-tau-max_steps+1ï¼‰
    for i in range(tau, tau + max_steps):
        features[:, i] = net(features[:, i - tau:i]).reshape(-1) # .reshape(-1)å±•å¹³ä¸ºä¸€ç»´å‘é‡

    steps = (1, 4, 16, 64)  # è¦å±•ç¤ºçš„é¢„æµ‹æ­¥æ•°
    common.plot([time[tau + i - 1: T - max_steps + i] for i in steps],
             [features[:, (tau + i - 1)].detach().numpy() for i in steps], 'time', 'x',
             legend=[f'{i}-step preds' for i in steps], xlim=[5, 1000],
             figsize=(6, 3))
# learn_SequenceModel()


# ä¸‹è½½å™¨ä¸æ•°æ®é›†é…ç½®
# ä¸º time_machine æ•°æ®é›†æ³¨å†Œä¸‹è½½ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ–‡ä»¶è·¯å¾„å’Œæ ¡éªŒå“ˆå¸Œå€¼ï¼ˆç”¨äºéªŒè¯æ–‡ä»¶å®Œæ•´æ€§ï¼‰
downloader = common.C_Downloader()
DATA_HUB = downloader.DATA_HUB  # å­—å…¸ï¼Œå­˜å‚¨æ•°æ®é›†åç§°ä¸ä¸‹è½½ä¿¡æ¯
DATA_URL = downloader.DATA_URL  # åŸºç¡€URLï¼ŒæŒ‡å‘æ•°æ®é›†çš„å­˜å‚¨ä½ç½®

DATA_HUB['time_machine'] = (DATA_URL + 'timemachine.txt',
                                '090b5e7e70c295757f55df93cb0a180b9691891a')


def learn_textPreprocess():
    # # åŠ è½½æ–‡æœ¬æ•°æ®
    # def read_time_machine():  #@save
    #     """å°†æ—¶é—´æœºå™¨æ•°æ®é›†åŠ è½½åˆ°æ–‡æœ¬è¡Œçš„åˆ—è¡¨ä¸­"""
    #     # é€šè¿‡ downloader.download('time_machine') è·å–æ–‡ä»¶è·¯å¾„
    #     with open(downloader.download('time_machine'), 'r') as f:
    #         lines = f.readlines() # é€è¡Œè¯»å–æ–‡æœ¬æ–‡ä»¶
    #     # ç”¨æ­£åˆ™è¡¨è¾¾å¼ [^A-Za-z]+ æ›¿æ¢æ‰€æœ‰éå­—æ¯å­—ç¬¦ä¸ºç©ºæ ¼
    #     # è°ƒç”¨ strip() å»é™¤é¦–å°¾ç©ºæ ¼ï¼Œlower() è½¬æ¢ä¸ºå°å†™
    #     # è¿”å›å€¼ï¼šå¤„ç†åçš„æ–‡æœ¬è¡Œåˆ—è¡¨ï¼ˆæ¯è¡Œæ˜¯çº¯å­—æ¯ç»„æˆçš„å­—ç¬¦ä¸²ï¼‰
    #     return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]

    lines = common.read_time_machine(downloader)
    print(f'# æ–‡æœ¬æ€»è¡Œæ•°: {len(lines)}')
    print(lines[0])     # ç¬¬1è¡Œå†…å®¹
    print(lines[10])    # ç¬¬11è¡Œå†…å®¹

    # # è¯å…ƒåŒ–å‡½æ•°ï¼šæ”¯æŒæŒ‰å•è¯æˆ–å­—ç¬¦æ‹†åˆ†æ–‡æœ¬
    # # linesï¼šé¢„å¤„ç†åçš„æ–‡æœ¬è¡Œåˆ—è¡¨
    # # tokenï¼šè¯å…ƒç±»å‹ï¼Œå¯é€‰ 'word'ï¼ˆé»˜è®¤ï¼‰æˆ– 'char
    # # è¿”å›å€¼ï¼šåµŒå¥—åˆ—è¡¨ï¼Œæ¯è¡Œå¯¹åº”ä¸€ä¸ªè¯å…ƒåˆ—è¡¨
    # def tokenize(lines, token='word'):  #@save
    #     """å°†æ–‡æœ¬è¡Œæ‹†åˆ†ä¸ºå•è¯æˆ–å­—ç¬¦è¯å…ƒ"""
    #     if token == 'word':
    #         return [line.split() for line in lines]  # æŒ‰ç©ºæ ¼åˆ†è¯
    #     elif token == 'char':
    #         return [list(line) for line in lines]   # æŒ‰å­—ç¬¦æ‹†åˆ†
    #     else:
    #         print('é”™è¯¯ï¼šæœªçŸ¥è¯å…ƒç±»å‹ï¼š' + token)

    tokens = common.tokenize(lines)
    for i in range(11):
        print(f"ç¬¬{i}è¡Œï¼š{tokens[i]}")

    '''
    å‡è®¾åŸå§‹æ–‡æœ¬å‰ä¸¤è¡Œä¸ºï¼š
    The Time Machine, by H. G. Wells [1898]
    I
    é¢„å¤„ç†åï¼š['the time machine by h g wells', 'i']
    è¯å…ƒåŒ–ç»“æœï¼š[['the', 'time', 'machine', 'by', 'h', 'g', 'wells'], ['i']]
    '''

    vocab = common.Vocab(tokens) # æ„å»ºè¯è¡¨ï¼Œç®¡ç†è¯å…ƒä¸ç´¢å¼•çš„æ˜ å°„å…³ç³»
    print(f"å‰å‡ ä¸ªé«˜é¢‘è¯åŠå…¶ç´¢å¼•ï¼š\n{list(vocab.token_to_idx.items())[:10]}")

    for i in [0, 10]: # å°†æ¯ä¸€æ¡æ–‡æœ¬è¡Œè½¬æ¢æˆä¸€ä¸ªæ•°å­—ç´¢å¼•åˆ—è¡¨
        print(f"ç¬¬{i}è¡Œä¿¡æ¯ï¼š")
        print('æ–‡æœ¬:', tokens[i])
        print('ç´¢å¼•:', vocab[tokens[i]])


    # # è·å–ã€Šæ—¶å…‰æœºå™¨ã€‹çš„ è¯å…ƒç´¢å¼•åºåˆ—å’Œè¯è¡¨å¯¹è±¡
    # # max_tokensï¼šé™åˆ¶è¿”å›çš„è¯å…ƒç´¢å¼•åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤ -1 è¡¨ç¤ºä¸é™åˆ¶ï¼‰
    # def load_corpus_time_machine(max_tokens=-1):  #@save
    #     """è¿”å›æ—¶å…‰æœºå™¨æ•°æ®é›†çš„è¯å…ƒç´¢å¼•åˆ—è¡¨å’Œè¯è¡¨"""
    #     lines = read_time_machine() # åŠ è½½æ–‡æœ¬æ•°æ®ï¼Œå¾—åˆ°æ–‡æœ¬è¡Œåˆ—è¡¨
    #     tokens = tokenize(lines, 'char') # è¯å…ƒåŒ–ï¼šæ–‡æœ¬è¡Œåˆ—è¡¨â†’è¯å…ƒåˆ—è¡¨ï¼ŒæŒ‰å­—ç¬¦çº§æ‹†åˆ†
    #     vocab = common.Vocab(tokens) # æ„å»ºè¯è¡¨
    #     # å› ä¸ºæ—¶å…‰æœºå™¨æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ–‡æœ¬è¡Œä¸ä¸€å®šæ˜¯ä¸€ä¸ªå¥å­æˆ–ä¸€ä¸ªæ®µè½ï¼Œ
    #     # æ‰€ä»¥å°†æ‰€æœ‰æ–‡æœ¬è¡Œå±•å¹³åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­
    #     # vocab[token] æŸ¥è¯¢è¯å…ƒçš„ç´¢å¼•ï¼ˆè‹¥è¯å…ƒä¸å­˜åœ¨ï¼Œåˆ™è¿”å›0ï¼Œå³æœªçŸ¥è¯ç´¢å¼•ï¼‰
    #     # corpusï¼šlistï¼Œæ¯ä¸ªå…ƒç´ ä¸ºè¯å…ƒçš„å¯¹åº”ç´¢å¼•
    #     corpus = [vocab[token] for line in tokens for token in line] # å±•å¹³è¯å…ƒå¹¶è½¬æ¢ä¸ºç´¢å¼•
    #     if max_tokens > 0: # é™åˆ¶è¯å…ƒåºåˆ—é•¿åº¦
    #         corpus = corpus[:max_tokens] # æˆªæ–­ corpus åˆ°å‰ max_tokens ä¸ªè¯å…ƒ
    #     # corpusï¼šè¯å…ƒç´¢å¼•åˆ—è¡¨ï¼ˆå¦‚ [1, 2, 3, ...]ï¼‰
    #     # vocabï¼šVocabå¯¹è±¡ï¼Œç”¨äºç®¡ç†è¯å…ƒä¸ç´¢å¼•çš„æ˜ å°„
    #     return corpus, vocab

    corpus, vocab = common.load_corpus_time_machine(downloader) # åŠ è½½æ•°æ®
    print(f"corpusè¯å…ƒç´¢å¼•åˆ—è¡¨çš„é•¿åº¦ï¼š{len(corpus)}")
    print(f"è¯è¡¨å¤§å°ï¼š{len(vocab)}")
    print(f"è¯é¢‘ç»Ÿè®¡ï¼ˆé™åºï¼‰ï¼š\n{vocab.token_freqs}")
    # ç´¢å¼• â†” è¯å…ƒè½¬æ¢
    print(f"å‰10ä¸ªç´¢å¼•å¯¹åº”çš„è¯å…ƒï¼š\n{vocab.to_tokens(corpus[:10])}")
    print(f"å‰10ä¸ªè¯å…ƒå¯¹åº”çš„ç´¢å¼•ï¼š\n{corpus[:10]}")
    print(f"å‰10ä¸ªè¯å…ƒå¯¹åº”çš„ç´¢å¼•ï¼š\n{[idx for idx in corpus[:10]]}")
# learn_textPreprocess()


def learn_languageModelsAndDatasets():
    lines = common.read_time_machine(downloader) # è·å–æ–‡æœ¬è¡Œåˆ—è¡¨
    tokens = common.tokenize(lines) # å°†æ–‡æœ¬è¡Œåˆ—è¡¨ä¸­çš„å…ƒç´ è¯å…ƒåŒ–(æŒ‰å•è¯æ‹†åˆ†)
    # å› ä¸ºæ¯ä¸ªæ–‡æœ¬è¡Œä¸ä¸€å®šæ˜¯ä¸€ä¸ªå¥å­æˆ–ä¸€ä¸ªæ®µè½ï¼Œå› æ­¤æŠŠæ‰€æœ‰æ–‡æœ¬è¡Œæ‹¼æ¥åˆ°ä¸€èµ·
    corpus = [token for line in tokens for token in line] # å°†è¯å…ƒåˆ—è¡¨å±•å¹³
    vocab = common.Vocab(corpus)
    print(f"å‰10ä¸ªæœ€å¸¸ç”¨çš„ï¼ˆé¢‘ç‡æœ€é«˜çš„ï¼‰å•è¯ï¼š\n{vocab.token_freqs[:10]}")

    freqs = [freq for token, freq in vocab.token_freqs] # è¯é¢‘(é™åº)
    common.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',
             xscale='log', yscale='log') # ç»˜åˆ¶(æ¨ªåæ ‡=è¯é¢‘ç´¢å¼•ï¼Œçºµåæ ‡=è¯é¢‘å…·ä½“æ•°å€¼)

    # è¯å…ƒç»„åˆ(äºŒå…ƒè¯­æ³•)
    bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]
    bigram_vocab = common.Vocab(bigram_tokens)
    print(f"å‰10ä¸ªæœ€å¸¸ç”¨çš„ï¼ˆé¢‘ç‡æœ€é«˜çš„ï¼‰è¯å…ƒç»„åˆ(äºŒå…ƒè¯­æ³•)ï¼š\n{bigram_vocab.token_freqs[:10]}")

    # è¯å…ƒç»„åˆ(ä¸‰å…ƒè¯­æ³•)
    trigram_tokens = [triple for triple in zip(corpus[:-2], corpus[1:-1], corpus[2:])]
    trigram_vocab = common.Vocab(trigram_tokens)
    print(f"å‰10ä¸ªæœ€å¸¸ç”¨çš„ï¼ˆé¢‘ç‡æœ€é«˜çš„ï¼‰è¯å…ƒç»„åˆ(ä¸‰å…ƒè¯­æ³•)ï¼š\n{trigram_vocab.token_freqs[:10]}")

    # å†ç›´è§‚å¯¹æ¯” ä¸‰ç§æ¨¡å‹ä¸­çš„è¯å…ƒé¢‘ç‡ï¼šä¸€å…ƒè¯­æ³•ã€äºŒå…ƒè¯­æ³•å’Œä¸‰å…ƒè¯­æ³•
    bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]
    trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]
    common.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',
             ylabel='frequency: n(x)', xscale='log', yscale='log',
             legend=['unigram', 'bigram', 'trigram'])


    my_seq = list(range(35)) # ç”Ÿæˆä¸€ä¸ªä»0åˆ°34çš„åºåˆ—
    # æ‰¹é‡å¤§å°ä¸º2ï¼Œæ—¶é—´æ­¥æ•°ä¸º5
    for idx, (X, Y) in enumerate(common.seq_data_iter_random(my_seq, batch_size=2, num_steps=5)):
        print(f" éšæœºå–æ · â€”â€”â€”â€”â€”â€” idx={idx} â€”â€”â€”â€”â€”â€” \n"
              f"X: {X}\nY:{Y}")

    for idx, (X, Y) in enumerate(common.seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5)):
        print(f" é¡ºåºåˆ†åŒº â€”â€”â€”â€”â€”â€” idx={idx} â€”â€”â€”â€”â€”â€” \n"
              f"X: {X}\nY:{Y}")
# learn_languageModelsAndDatasets()


# éªŒè¯ï¼šåˆ†åˆ«çŸ©é˜µä¹˜æ³•åå†ç»“æœç›¸åŠ  ç›¸å½“äº è¾“å…¥å’Œæƒé‡åˆ†åˆ«æ‹¼æ¥åå†çŸ©é˜µä¹˜æ³•
def learn_422():
    # ï¼ˆ1ï¼‰åˆ†åˆ«çŸ©é˜µä¹˜æ³•å å†ç»“æœç›¸åŠ 
    # é¦–å…ˆå®šä¹‰çŸ©é˜µ Xã€W_xhã€H å’Œ W_hhï¼Œåˆ†åˆ«ä¸º(3,1)ã€(1,4)ã€(3,4)å’Œ(4,4)
    # .normal() ä»ç¦»æ•£æ­£æ€åˆ†å¸ƒ(å‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1)ä¸­æŠ½å–éšæœºæ•°
    X, W_xh = torch.normal(0, 1, (3, 1)), torch.normal(0, 1, (1, 4))
    H, W_hh = torch.normal(0, 1, (3, 4)), torch.normal(0, 1, (4, 4))
    # åˆ†åˆ«å°†Xä¹˜ä»¥W_xhï¼Œå°†Hä¹˜ä»¥W_hhï¼Œç„¶åå°†è¿™ä¸¤ä¸ªä¹˜æ³•ç›¸åŠ 
    temp_add = torch.matmul(X, W_xh) + torch.matmul(H, W_hh)
    print(f"å¾—åˆ°ä¸€ä¸ªå½¢çŠ¶ä¸º(3,4)çš„çŸ©é˜µï¼š\n{temp_add}")

    # ï¼ˆ2ï¼‰è¾“å…¥å’Œæƒé‡åˆ†åˆ«æ‹¼æ¥å å†çŸ©é˜µä¹˜æ³•
    # .cat((X, H), 1) æ²¿åˆ—ï¼ˆè½´1ï¼‰æ‹¼æ¥çŸ©é˜µXå’ŒH
    # .cat((W_xh, W_hh), 0)) æ²¿è¡Œï¼ˆè½´0ï¼‰æ‹¼æ¥çŸ©é˜µW_xhå’ŒW_hh
    # è¿™ä¸¤ä¸ªæ‹¼æ¥åˆ†åˆ«äº§ç”Ÿå½¢çŠ¶(3,5)å’Œå½¢çŠ¶(5,4)çš„çŸ©é˜µ
    all_input = torch.cat((X, H), 1)
    all_w = torch.cat((W_xh, W_hh), 0)
    print(f"æ‹¼æ¥åçš„è¾“å…¥ï¼š\n{all_input}")
    print(f"æ‹¼æ¥åçš„æƒé‡ï¼š\n{all_w}")
    temp_add = torch.matmul(all_input, all_w) # å†å°†è¿™ä¸¤ä¸ªæ‹¼æ¥çš„çŸ©é˜µç›¸ä¹˜
    print(f"å¾—åˆ°ä¸ä¸Šé¢ç›¸åŒå½¢çŠ¶(3,4)çš„è¾“å‡ºçŸ©é˜µï¼š\n{temp_add}")
# def learn_422()






# å¾ªç¯ç¥ç»ç½‘ç»œçš„ä»é›¶å¼€å§‹å®ç°
def learn_rnn_StartFromScratch():
    batch_size, num_steps = 32, 35 # æ¯ä¸ªå°æ‰¹é‡åŒ…å«32ä¸ªå­åºåˆ—ï¼Œæ¯ä¸ªå­åºåˆ—çš„è¯å…ƒæ•°ä¸º35
    train_iter, vocab = common.load_data_time_machine(downloader, batch_size, num_steps) # è¯è¡¨å¯¹è±¡

    # å°†ç´¢å¼• [0, 2] è½¬æ¢ä¸ºé•¿åº¦ä¸º len(vocab) çš„ one-hot ç¼–ç ï¼Œ
    # ç»“æœæ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º (2, len(vocab)) çš„å¼ é‡ï¼Œå…¶ä¸­æ¯ä¸€è¡Œæ˜¯å¯¹åº”ç´¢å¼•çš„ one-hot å‘é‡
    F.one_hot(torch.tensor([0, 2]), len(vocab))
    print(f"ç´¢å¼•ä¸º0å’Œ2çš„ç‹¬çƒ­å‘é‡ï¼š\n{F.one_hot(torch.tensor([0, 2]), len(vocab))}")

    X = torch.arange(10).reshape((2, 5)) # åˆ›å»ºå½¢çŠ¶ä¸º (2, 5) çš„å¼ é‡ Xï¼ŒåŒ…å« 0~9 çš„æ•´æ•°
    # X.T è½¬ç½®Xï¼Œå¾—åˆ°å½¢çŠ¶ (5, 2)ï¼Œè¡¨ç¤º5ä¸ªæ—¶é—´æ­¥ï¼ˆæˆ–åºåˆ—é•¿åº¦ï¼‰ï¼Œæ¯ä¸ªæ—¶é—´æ­¥æœ‰2ä¸ªç‰¹å¾ï¼ˆæˆ–2ä¸ªç‹¬ç«‹çš„ç´¢å¼•ï¼‰
    # one_hot(X.T, 28) å¯¹X.Tåº”ç”¨one_hotç¼–ç ï¼Œnum_classes=28 (5ä¸ªæ—¶é—´æ­¥ Ã— 2ä¸ªç´¢å¼• Ã— æ¯ä¸ªç´¢å¼•çš„28ç»´one-hotç¼–ç )
    print(f"{F.one_hot(X.T, 28).shape}")

    """
    åˆå§‹åŒ–RNNæ¨¡å‹çš„å‚æ•°
    æƒé‡ä½¿ç”¨å°éšæœºæ•°åˆå§‹åŒ–ï¼Œåç½®åˆå§‹åŒ–ä¸ºé›¶
    åŒ…æ‹¬ï¼šè¾“å…¥åˆ°éšè—å±‚ã€éšè—å±‚åˆ°éšè—å±‚ã€éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡ä»¥åŠç›¸åº”çš„åç½®
    å‚æ•°:
    vocab_size : è¯è¡¨å¤§å° (è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦)
    num_hiddens: éšè—å±‚å¤§å°
    device     : è®¡ç®—è®¾å¤‡ (CPU/GPU)
    è¿”å›:
    params: æ¨¡å‹å‚æ•°åˆ—è¡¨ [W_xh, W_hh, b_h, W_hq, b_q]
    """
    def get_params(vocab_size, num_hiddens, device): # è°ƒç”¨æ—¶ä¼ å…¥äº† 25ï¼Œ512
        num_inputs = num_outputs = vocab_size # è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦éƒ½æ˜¯è¯è¡¨å¤§å°

        def normal(shape):  # å®šä¹‰æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–å‡½æ•°
            return torch.randn(size=shape, device=device) * 0.01 # ç”Ÿæˆå°éšæœºæ•°

        # éšè—å±‚å‚æ•°
        W_xh = normal((num_inputs, num_hiddens))        # è¾“å…¥åˆ°éšè—å±‚çš„æƒé‡(28, 512)
        W_hh = normal((num_hiddens, num_hiddens))       # éšè—å±‚åˆ°éšè—å±‚çš„æƒé‡(512, 512)
        b_h = torch.zeros(num_hiddens, device=device)   # éšè—å±‚åç½®(512,)
        # è¾“å‡ºå±‚å‚æ•°
        W_hq = normal((num_hiddens, num_outputs))       # éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡(512, 28)
        b_q = torch.zeros(num_outputs, device=device)   # è¾“å‡ºå±‚åç½®(28,)
        # é™„åŠ æ¢¯åº¦
        params = [W_xh, W_hh, b_h, W_hq, b_q] # å°†æ‰€æœ‰å‚æ•°æ”¾å…¥åˆ—è¡¨
        for param in params: # è®¾ç½®ä¸ºéœ€è¦æ¢¯åº¦
            param.requires_grad_(True)  # å¼€å¯æ¢¯åº¦è¿½è¸ª
        return params # è¿”å›æ¨¡å‹å‚æ•°åˆ—è¡¨ [W_xh, W_hh, b_h, W_hq, b_q]

    '''
    åˆå§‹åŒ–RNNçš„éšè—çŠ¶æ€
    åŠŸèƒ½ï¼šè¿”å›åˆå§‹çš„éšè—çŠ¶æ€ï¼ˆä¸€ä¸ªå…¨é›¶å¼ é‡ï¼‰ï¼Œå½¢çŠ¶ä¸º (batch_size, num_hiddens)
    batch_size : æ‰¹é‡å¤§å°
    num_hiddens: éšè—å±‚å¤§å°
    device     : è®¡ç®—è®¾å¤‡
    è¿”å›:åŒ…å«åˆå§‹éšè—çŠ¶æ€çš„å…ƒç»„ (H0,)
    '''
    def init_rnn_state(batch_size, num_hiddens, device):
        # åˆ›å»ºå…¨é›¶çš„åˆå§‹éšè—çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º (batch_size, num_hiddens)
        return (torch.zeros((batch_size, num_hiddens), device=device), ) # åˆå§‹éšè—çŠ¶æ€ä¸ºå…¨0

    '''
    RNNå‰å‘ä¼ æ’­å‡½æ•°
    inputs: è¾“å…¥åºåˆ—ï¼Œå½¢çŠ¶ä¸º (æ—¶é—´æ­¥æ•°é‡, æ‰¹é‡å¤§å°, è¯è¡¨å¤§å°)
    state : åˆå§‹éšè—çŠ¶æ€çš„å…ƒç»„ (H0,)
    params: æ¨¡å‹å‚æ•°åˆ—è¡¨ [W_xh, W_hh, b_h, W_hq, b_q]
    è¿”å›:
    outputs  : æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡ºï¼Œå½¢çŠ¶ä¸º (æ—¶é—´æ­¥æ•°é‡ * æ‰¹é‡å¤§å°, è¯è¡¨å¤§å°)
    new_state: æ–°çš„éšè—çŠ¶æ€
    '''
    def rnn(inputs, state, params):
        # inputsçš„å½¢çŠ¶ï¼š(æ—¶é—´æ­¥æ•°é‡ï¼Œæ‰¹é‡å¤§å°ï¼Œè¯è¡¨å¤§å°)
        W_xh, W_hh, b_h, W_hq, b_q = params # è§£åŒ…å‚æ•°
        H, = state                          # è§£åŒ…éšè—çŠ¶æ€
        outputs = []                        # ç”¨äºå­˜å‚¨æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        # Xçš„å½¢çŠ¶ï¼š(æ‰¹é‡å¤§å°ï¼Œè¯è¡¨å¤§å°)
        for X in inputs:# éå†æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥
            # ä½¿ç”¨tanhä½œä¸ºæ¿€æ´»å‡½æ•°
            # è®¡ç®—æ–°çš„éšè—çŠ¶æ€ï¼šH = tanh(X * W_xh + H * W_hh + b_h)
            H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)
            Y = torch.mm(H, W_hq) + b_q # è®¡ç®—å½“å‰æ—¶é—´æ­¥çš„è¾“å‡ºï¼šY = H * W_hq + b_q
            outputs.append(Y)  # ä¿å­˜è¾“å‡º
        # å°†æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡ºæ²¿æ—¶é—´ç»´åº¦æ‹¼æ¥
        # (H,) è¡¨ç¤ºåªåŒ…å«ä¸€ä¸ªå…ƒç´ Hçš„å…ƒç»„ï¼ˆtupleï¼‰ï¼Œä½†çœŸæ­£å®šä¹‰å…ƒç»„çš„æ˜¯é€—å·ï¼Œè€Œä¸æ˜¯åœ†æ‹¬å·
        # å¦‚æœå†™(H)ï¼Œè¿™åªæ˜¯å¸¦æ‹¬å·çš„è¡¨è¾¾å¼ï¼Œç­‰åŒäº`H`ï¼Œè€Œä¸æ˜¯å…ƒç»„ã€‚ä¸ºäº†è¡¨ç¤ºè¿™æ˜¯åªæœ‰ä¸€ä¸ªå…ƒç´ çš„å…ƒç»„ï¼Œéœ€åœ¨å…ƒç´ ååŠ é€—å·ã€‚æ‰€ä»¥ï¼š
        # (H)  æ˜¯ä¸€ä¸ªè¡¨è¾¾å¼ï¼Œå…¶å€¼ä¸ºH
        # (H,) æ˜¯ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ä¸€ä¸ªå…ƒç´ H
        return torch.cat(outputs, dim=0), (H,) # è¿”å›è¾“å‡ºå’Œæœ€åä¸€ä¸ªéšè—çŠ¶æ€

    '''
    RNNæ¨¡å‹ç±»
    åŠŸèƒ½ï¼šå®ç°RNNçš„å‰å‘ä¼ æ’­
    å¯¹è¾“å…¥åºåˆ—çš„æ¯ä¸ªæ—¶é—´æ­¥ï¼Œè®¡ç®—éšè—çŠ¶æ€å’Œè¾“å‡º
    è¾“å…¥inputså½¢çŠ¶ä¸º (æ—¶é—´æ­¥æ•°, æ‰¹é‡å¤§å°, è¯è¡¨å¤§å°)ï¼ˆå®é™…åœ¨è°ƒç”¨å‰ä¼šè½¬æˆone-hotï¼‰
    è¯¥å‡½æ•°è¿”å›æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡ºï¼ˆæ‹¼æ¥æˆä¸€ä¸ªå¼ é‡ï¼‰å’Œæœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€
    '''
    class RNNModelScratch: #@save
        """ä»é›¶å¼€å§‹å®ç°çš„å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹"""
        def __init__(self, vocab_size, num_hiddens, device,
                     get_params, init_state, forward_fn):
            self.vocab_size, self.num_hiddens = vocab_size, num_hiddens # è¯è¡¨å¤§å°,éšè—å±‚å¤§å°
            self.params = get_params(vocab_size, num_hiddens, device)   # åˆå§‹åŒ–å‚æ•°
            self.init_state, self.forward_fn = init_state, forward_fn   # åˆå§‹åŒ–éšè—çŠ¶æ€çš„å‡½æ•°,å‰å‘ä¼ æ’­å‡½æ•°

        """
        æ¨¡å‹è°ƒç”¨æ–¹æ³•ï¼ˆå‰å‘ä¼ æ’­ï¼‰
        å‚æ•°:
        X: è¾“å…¥åºåˆ—ï¼Œå½¢çŠ¶ä¸º (æ‰¹é‡å¤§å°, æ—¶é—´æ­¥æ•°é‡)ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯è¯ç´¢å¼•ï¼ˆæ•´æ•°ï¼‰
        state: éšè—çŠ¶æ€
        è¿”å›:è¾“å‡ºå’Œæ–°çš„éšè—çŠ¶æ€
        __call__å®ç°åï¼Œè¯¥ç±»çš„å®ä¾‹å°±å¯ä»¥è¢«å½“ä½œå‡½æ•°ä½¿ç”¨ï¼Œ
        å³é€šè¿‡`instance(arguments)`çš„æ–¹å¼è°ƒç”¨è§¦å‘`__call__`æ–¹æ³•çš„æ‰§è¡Œ
        è¿™é‡Œï¼šåˆ›å»ºRNNModelScratchçš„å®ä¾‹ï¼ˆä¾‹å¦‚netï¼‰åï¼Œå¯ä»¥åƒå‡½æ•°ä¸€æ ·è°ƒç”¨è¿™ä¸ªå®ä¾‹ğŸ‘‡
        net = RNNModelScratch(len(vocab), num_hiddens, common.try_gpu(), get_params,
                            init_rnn_state, rnn) # åˆ›å»ºæ¨¡å‹å®ä¾‹
        """
        def __call__(self, X, state):
            # å°†è¾“å…¥Xè½¬æ¢ä¸ºone-hotç¼–ç 
            # X.T: è½¬ç½®ä¸º (æ—¶é—´æ­¥æ•°é‡, æ‰¹é‡å¤§å°)
            # one_hot: è½¬æ¢ä¸º (æ—¶é—´æ­¥æ•°é‡, æ‰¹é‡å¤§å°, è¯è¡¨å¤§å°)
            X = F.one_hot(X.T, self.vocab_size).type(torch.float32)
            return self.forward_fn(X, state, self.params) # è°ƒç”¨å‰å‘ä¼ æ’­å‡½æ•°

        def begin_state(self, batch_size, device):
            """è·å–åˆå§‹éšè—çŠ¶æ€"""
            return self.init_state(batch_size, self.num_hiddens, device) # è¿”å›åˆå§‹éšè—çŠ¶æ€

    num_hiddens = 512 # éšè—å±‚å¤§å°
    net = RNNModelScratch(len(vocab), num_hiddens, common.try_gpu(), get_params,
                          init_rnn_state, rnn) # åˆ›å»ºæ¨¡å‹å®ä¾‹
    state = net.begin_state(X.shape[0], common.try_gpu()) # è·å–åˆå§‹çŠ¶æ€ï¼ˆå‡è®¾Xæ˜¯ä¸€ä¸ªæ‰¹é‡çš„è¾“å…¥æ•°æ®ï¼‰
    Y, new_state = net(X.to(common.try_gpu()), state) # å°†æ•°æ®Xè½¬ç§»åˆ°è®¾å¤‡ï¼ˆå¦‚GPUï¼‰å¹¶å‰å‘ä¼ æ’­(æŠŠç±»å½“ä½œå‡½æ•°ä½¿ç”¨ï¼Œè°ƒç”¨__call__)
    print(f"è¾“å‡ºYçš„å½¢çŠ¶ï¼š{Y.shape}") # (æ—¶é—´æ­¥æ•°é‡ * æ‰¹é‡å¤§å°, è¯è¡¨å¤§å°)
    print(f"éšè—çŠ¶æ€çš„å…ƒç»„é•¿åº¦ï¼š{len(new_state)}")  # éšè—çŠ¶æ€çš„å…ƒç»„é•¿åº¦: 1
    print(f"éšè—çŠ¶æ€çš„å½¢çŠ¶ï¼š{new_state[0].shape}") # (æ‰¹é‡å¤§å°, éšè—å±‚å¤§å°)


    print(f"æœªè®­ç»ƒç½‘ç»œçš„æƒ…å†µä¸‹ï¼Œæµ‹è¯•å‡½æ•°åŸºäºtime travellerè¿™ä¸ªå‰ç¼€ç”Ÿæˆ10ä¸ªåç»­å­—ç¬¦ï¼š\n"
          f"{common.predict_ch8('time traveller ', 10, net, vocab, common.try_gpu())}")

    num_epochs, lr = 500, 1 # è¿­ä»£å‘¨æœŸä¸º500ï¼Œå³è®­ç»ƒ500è½®ï¼›å­¦ä¹ ç‡ä¸º1
    common.train_ch8(net, train_iter, vocab, lr, num_epochs, common.try_gpu())

    # é‡æ–°åˆå§‹åŒ–ä¸€ä¸ªRNNæ¨¡å‹
    net = RNNModelScratch(len(vocab), num_hiddens, common.try_gpu(), get_params,
                          init_rnn_state, rnn)
    # ä½¿ç”¨éšæœºæŠ½æ ·è®­ç»ƒæ¨¡å‹
    common.train_ch8(net, train_iter, vocab, lr, num_epochs, common.try_gpu(),
              use_random_iter=True)
# learn_rnn_StartFromScratch()


# å¾ªç¯ç¥ç»ç½‘ç»œçš„ç®€æ´å®ç°
def learn_rnn_SimpleImplementation():
    batch_size, num_steps = 32, 35 # æ¯ä¸ªå°æ‰¹é‡åŒ…å«32ä¸ªå­åºåˆ—ï¼Œæ¯ä¸ªå­åºåˆ—çš„è¯å…ƒæ•°ä¸º35
    train_iter, vocab = common.load_data_time_machine(downloader, batch_size, num_steps) # è¯è¡¨å¯¹è±¡

    num_hiddens = 256 # éšè—å•å…ƒæ•°é‡ï¼Œå³ æœ‰256ä¸ªéšè—å•å…ƒï¼Œrnnæœ‰256ä¸ªç¥ç»å…ƒ

    # è¾“å…¥ç»´åº¦: len(vocab) (è¯è¡¨å¤§å°)
    # éšè—å±‚ç»´åº¦: num_hiddens (256)
    # é»˜è®¤ä½¿ç”¨tanhæ¿€æ´»å‡½æ•°ï¼Œå•å±‚å•å‘RNN
    rnn_layer = nn.RNN(len(vocab), num_hiddens) # åˆ›å»ºRNNå±‚

    # å½¢çŠ¶: (å±‚æ•° * æ–¹å‘æ•°, æ‰¹é‡å¤§å°, éšè—å•å…ƒæ•°)
    # å¯¹äºå•å±‚å•å‘RNN: å±‚æ•°=1, æ–¹å‘æ•°=1
    state = torch.zeros((1, batch_size, num_hiddens)) # åˆå§‹åŒ–éšè—çŠ¶æ€
    print(f"åˆå§‹åŒ–éšçŠ¶æ€ï¼Œå®ƒçš„å½¢çŠ¶ï¼š{state.shape}") # è¾“å‡º: (1, batch_size, 256)

    # åˆ›å»ºéšæœºè¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶: (æ—¶é—´æ­¥æ•°, æ‰¹é‡å¤§å°, è¾“å…¥ç»´åº¦)
    X = torch.rand(size=(num_steps, batch_size, len(vocab)))
    # rnn_layerçš„ â€œè¾“å‡ºâ€Y ä¸æ¶‰åŠè¾“å‡ºå±‚çš„è®¡ç®—ï¼š
    # å®ƒæ˜¯æŒ‡æ¯ä¸ªæ—¶é—´æ­¥çš„éšçŠ¶æ€ï¼Œè¿™äº›éšçŠ¶æ€å¯ä»¥ç”¨ä½œåç»­è¾“å‡ºå±‚çš„è¾“å…¥
    Y, state_new = rnn_layer(X, state)
    print(f"è¾“å‡ºYçš„å½¢çŠ¶ï¼š{Y.shape}") # (æ—¶é—´æ­¥æ•°,æ‰¹é‡å¤§å°,éšè—å•å…ƒæ•°)->(num_steps, batch_size, 256)
    print(f"æ–°çš„éšè—çŠ¶æ€ï¼Œå…¶å½¢çŠ¶ï¼š{state_new.shape}") # (1, batch_size, 256)

    # å®šä¹‰å®Œæ•´çš„RNNæ¨¡å‹ç±»
    class RNNModel(nn.Module):
        """å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹"""
        def __init__(self, rnn_layer, vocab_size, **kwargs):
            super(RNNModel, self).__init__(**kwargs)
            self.rnn = rnn_layer                    # ä¼ å…¥çš„RNNå±‚
            self.vocab_size = vocab_size            # è¯è¡¨å¤§å°
            self.num_hiddens = self.rnn.hidden_size # ä»RNNå±‚è·å–éšè—å•å…ƒæ•°

            # åˆ¤æ–­RNNæ˜¯å¦ä¸ºåŒå‘ï¼šè‹¥RNNæ˜¯åŒå‘(ä¹‹åå°†ä»‹ç»)ï¼Œnum_directionsä¸º2ï¼Œå¦åˆ™ä¸º1
            if not self.rnn.bidirectional: # å•å‘RNN: num_directions = 1
                self.num_directions = 1
                # çº¿æ€§å±‚: å°†éšè—çŠ¶æ€æ˜ å°„åˆ°è¯è¡¨å¤§å°
                self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
            else:                          # åŒå‘RNN: num_directions = 2
                self.num_directions = 2
                # å¯¹äºåŒå‘RNNï¼Œéœ€è¦å°†ä¸¤ä¸ªæ–¹å‘çš„éšè—çŠ¶æ€æ‹¼æ¥
                self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)

        """ å‰å‘ä¼ æ’­å‡½æ•°
        inputs: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º(æ‰¹é‡å¤§å°, æ—¶é—´æ­¥æ•°)
        state: éšè—çŠ¶æ€
        """
        def forward(self, inputs, state):
            # è¾“å…¥è½¬ä¸ºone-hotç¼–ç 
            # inputs.T: è½¬ç½®ä¸º(æ—¶é—´æ­¥æ•°, æ‰¹é‡å¤§å°)
            # one-hotç¼–ç åå½¢çŠ¶: (æ—¶é—´æ­¥æ•°, æ‰¹é‡å¤§å°, è¯è¡¨å¤§å°)
            X = F.one_hot(inputs.T.long(), self.vocab_size)
            X = X.to(torch.float32)         # ç‹¬çƒ­å¼ é‡è½¬æ¢ä¸ºfloat32 (PyTorchçš„çº¿æ€§å±‚éœ€è¦æµ®ç‚¹è¾“å…¥)

            # Y: æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡ºï¼Œå½¢çŠ¶ä¸º(æ—¶é—´æ­¥æ•°, æ‰¹é‡å¤§å°, éšè—å•å…ƒæ•° * æ–¹å‘æ•°)
            # state: æ›´æ–°åçš„éšè—çŠ¶æ€
            Y, state = self.rnn(X, state)   # é€šè¿‡RNNå±‚

            # å…¨è¿æ¥å±‚é¦–å…ˆå°†Yçš„å½¢çŠ¶æ”¹ä¸º(æ—¶é—´æ­¥æ•°*æ‰¹é‡å¤§å°,éšè—å•å…ƒæ•°)
            # å³ é‡å¡‘Yçš„å½¢çŠ¶ä¸º (æ—¶é—´æ­¥æ•° * æ‰¹é‡å¤§å°, éšè—å•å…ƒæ•° * æ–¹å‘æ•°)
            # è¿™æ ·æ¯ä¸ªæ—¶é—´æ­¥çš„æ¯ä¸ªæ ·æœ¬éƒ½å¯ä»¥ç‹¬ç«‹å¤„ç†
            # å…¶è¾“å‡ºå½¢çŠ¶ä¸º (æ—¶é—´æ­¥æ•°*æ‰¹é‡å¤§å°, è¯è¡¨å¤§å°)
            output = self.linear(Y.reshape((-1, Y.shape[-1]))) # é€šè¿‡å…¨è¿æ¥å±‚è¿›è¡Œé¢„æµ‹
            return output, state

        # åˆå§‹åŒ–éšè—çŠ¶æ€å‡½æ•°ï¼šæ ¹æ®RNNç±»å‹(GRU/LSTM)è¿”å›é€‚å½“å½¢å¼çš„åˆå§‹çŠ¶æ€
        def begin_state(self, device, batch_size=1):
            if not isinstance(self.rnn, nn.LSTM): # å¯¹äºGRUç±»å‹çš„RNN
                # nn.GRUä»¥å¼ é‡ä½œä¸ºéšçŠ¶æ€
                # è¿”å›é›¶å¼ é‡ä½œä¸ºåˆå§‹çŠ¶æ€ï¼Œå½¢çŠ¶: (å±‚æ•° * æ–¹å‘æ•°, æ‰¹é‡å¤§å°, éšè—å•å…ƒæ•°)
                return  torch.zeros((self.num_directions * self.rnn.num_layers,
                                     batch_size, self.num_hiddens),
                                    device=device)
            else: # å¯¹äºLSTMç±»å‹çš„RNN
                # nn.LSTMä»¥å…ƒç»„ä½œä¸ºéšçŠ¶æ€ï¼ŒLSTMéœ€è¦ä¸¤ä¸ªçŠ¶æ€: (éšè—çŠ¶æ€h, ç»†èƒçŠ¶æ€c)
                return (
                    torch.zeros(( # éšè—çŠ¶æ€
                    self.num_directions * self.rnn.num_layers,
                    batch_size, self.num_hiddens), device=device),
                    torch.zeros(( # ç»†èƒçŠ¶æ€
                            self.num_directions * self.rnn.num_layers,
                            batch_size, self.num_hiddens), device=device))

    device = common.try_gpu()
    net = RNNModel(rnn_layer, vocab_size=len(vocab)) # åˆ›å»ºrnnæ¨¡å‹å®ä¾‹
    net = net.to(device)
    # å…ˆåŸºäºä¸€ä¸ªå…·æœ‰éšæœºæƒé‡çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼šåŸºäºèµ·å§‹å­—ç¬¦ç”Ÿæˆ10ä¸ªåç»­å­—ç¬¦
    # ï¼ˆæ­¤æ—¶æ¨¡å‹å°šæœªè®­ç»ƒï¼Œé¢„æµ‹æ˜¯éšæœºçš„ï¼‰
    output = common.predict_ch8('time traveller', 10, net, vocab, device)
    print(f"æœªè®­ç»ƒç½‘ç»œçš„æƒ…å†µä¸‹ï¼Œæµ‹è¯•å‡½æ•°åŸºäºtime travellerè¿™ä¸ªå‰ç¼€ç”Ÿæˆ10ä¸ªåç»­å­—ç¬¦ï¼š\n"
          f"{output}")

    # ä½¿ç”¨ä¸ä»é›¶å¼€å§‹å®ç°ä¸­çš„åŒæ¬¾è¶…å‚æ•°è®­ç»ƒï¼Œç”¨é«˜çº§apiè®­ç»ƒæ¨¡å‹
    num_epochs, lr = 500, 1 # è®­ç»ƒ500è½®ï¼Œå­¦ä¹ ç‡=1
    common.train_ch8(net, train_iter, vocab, lr, num_epochs, device)
learn_rnn_SimpleImplementation()

