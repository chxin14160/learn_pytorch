import torch
from torch import nn
import common
from torch.nn import functional as F


# ä¸‹è½½å™¨ä¸æ•°æ®é›†é…ç½®
# ä¸º time_machine æ•°æ®é›†æ³¨å†Œä¸‹è½½ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ–‡ä»¶è·¯å¾„å’Œæ ¡éªŒå“ˆå¸Œå€¼ï¼ˆç”¨äºéªŒè¯æ–‡ä»¶å®Œæ•´æ€§ï¼‰
downloader = common.C_Downloader()
DATA_HUB = downloader.DATA_HUB  # å­—å…¸ï¼Œå­˜å‚¨æ•°æ®é›†åç§°ä¸ä¸‹è½½ä¿¡æ¯
DATA_URL = downloader.DATA_URL  # åŸºç¡€URLï¼ŒæŒ‡å‘æ•°æ®é›†çš„å­˜å‚¨ä½ç½®

DATA_HUB['time_machine'] = (DATA_URL + 'timemachine.txt',
                                '090b5e7e70c295757f55df93cb0a180b9691891a')


batch_size, num_steps = 32, 35  # æ¯ä¸ªå°æ‰¹é‡åŒ…å«32ä¸ªå­åºåˆ—ï¼Œæ¯ä¸ªå­åºåˆ—çš„è¯å…ƒæ•°ä¸º35
train_iter, vocab = common.load_data_time_machine(downloader, batch_size, num_steps)  # è¯è¡¨å¯¹è±¡


'''
RNNæ¨¡å‹ç±»
åŠŸèƒ½ï¼šå®ç°RNNçš„å‰å‘ä¼ æ’­
å¯¹è¾“å…¥åºåˆ—çš„æ¯ä¸ªæ—¶é—´æ­¥ï¼Œè®¡ç®—éšè—çŠ¶æ€å’Œè¾“å‡º
è¾“å…¥inputså½¢çŠ¶ä¸º (æ—¶é—´æ­¥æ•°, æ‰¹é‡å¤§å°, è¯è¡¨å¤§å°)ï¼ˆå®é™…åœ¨è°ƒç”¨å‰ä¼šè½¬æˆone-hotï¼‰
è¯¥å‡½æ•°è¿”å›æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡ºï¼ˆæ‹¼æ¥æˆä¸€ä¸ªå¼ é‡ï¼‰å’Œæœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€
'''
class RNNModelScratch: #@save
    """ä»é›¶å¼€å§‹å®ç°çš„å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹"""
    def __init__(self, vocab_size, num_hiddens, device,
                 get_params, init_state, forward_fn):
        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens # è¯è¡¨å¤§å°,éšè—å±‚å¤§å°
        self.params = get_params(vocab_size, num_hiddens, device)   # åˆå§‹åŒ–å‚æ•°
        self.init_state, self.forward_fn = init_state, forward_fn   # åˆå§‹åŒ–éšè—çŠ¶æ€çš„å‡½æ•°,å‰å‘ä¼ æ’­å‡½æ•°

    """
    æ¨¡å‹è°ƒç”¨æ–¹æ³•ï¼ˆå‰å‘ä¼ æ’­ï¼‰
    å‚æ•°:
    X: è¾“å…¥åºåˆ—ï¼Œå½¢çŠ¶ä¸º (æ‰¹é‡å¤§å°, æ—¶é—´æ­¥æ•°é‡)ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯è¯ç´¢å¼•ï¼ˆæ•´æ•°ï¼‰
    state: éšè—çŠ¶æ€
    è¿”å›:è¾“å‡ºå’Œæ–°çš„éšè—çŠ¶æ€
    __call__å®ç°åï¼Œè¯¥ç±»çš„å®ä¾‹å°±å¯ä»¥è¢«å½“ä½œå‡½æ•°ä½¿ç”¨ï¼Œ
    å³é€šè¿‡`instance(arguments)`çš„æ–¹å¼è°ƒç”¨è§¦å‘`__call__`æ–¹æ³•çš„æ‰§è¡Œ
    è¿™é‡Œï¼šåˆ›å»ºRNNModelScratchçš„å®ä¾‹ï¼ˆä¾‹å¦‚netï¼‰åï¼Œå¯ä»¥åƒå‡½æ•°ä¸€æ ·è°ƒç”¨è¿™ä¸ªå®ä¾‹ğŸ‘‡
    net = RNNModelScratch(len(vocab), num_hiddens, common.try_gpu(), get_params,
                        init_rnn_state, rnn) # åˆ›å»ºæ¨¡å‹å®ä¾‹
    """
    def __call__(self, X, state):
        # å°†è¾“å…¥Xè½¬æ¢ä¸ºone-hotç¼–ç 
        # X.T: è½¬ç½®ä¸º (æ—¶é—´æ­¥æ•°é‡, æ‰¹é‡å¤§å°)
        # one_hot: è½¬æ¢ä¸º (æ—¶é—´æ­¥æ•°é‡, æ‰¹é‡å¤§å°, è¯è¡¨å¤§å°)
        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)
        return self.forward_fn(X, state, self.params) # è°ƒç”¨å‰å‘ä¼ æ’­å‡½æ•°

    def begin_state(self, batch_size, device):
        """è·å–åˆå§‹éšè—çŠ¶æ€"""
        return self.init_state(batch_size, self.num_hiddens, device) # è¿”å›åˆå§‹éšè—çŠ¶æ€


# é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰çš„ä»é›¶å¼€å§‹å®ç°
def learn_gru_StartFromScratch():
    # 1ã€åˆå§‹åŒ–æ¨¡å‹å‚æ•°
    """
    åˆå§‹åŒ– é—¨æ§å¾ªç¯å•å…ƒGRU çš„æ¨¡å‹å‚æ•°
    æƒé‡ä½¿ç”¨å°éšæœºæ•°åˆå§‹åŒ–ï¼Œåç½®åˆå§‹åŒ–ä¸º0
    åŒ…æ‹¬ï¼šè¾“å…¥åˆ°éšè—å±‚ã€éšè—å±‚åˆ°éšè—å±‚ã€éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡ä»¥åŠç›¸åº”çš„åç½®
    å‚æ•°:
    vocab_size : è¯è¡¨å¤§å° (è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦)
    num_hiddens: éšè—å±‚å¤§å°
    device     : è®¡ç®—è®¾å¤‡ (CPU/GPU)
    è¿”å›:
    params: åŒ…å«æ‰€æœ‰å‚æ•°çš„åˆ—è¡¨
    """
    def get_params(vocab_size, num_hiddens, device):
        num_inputs = num_outputs = vocab_size # è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦éƒ½æ˜¯è¯è¡¨å¤§å°ï¼ˆå­—ç¬¦çº§ï¼‰

        def normal(shape):  # å®šä¹‰æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–å‡½æ•°
            """ç”Ÿæˆæœä»æ­£æ€åˆ†å¸ƒçš„éšæœºå¼ é‡"""
            # ç”Ÿæˆæœä»æ­£æ€åˆ†å¸ƒçš„éšæœºå‚æ•°ï¼Œä¹˜ä»¥0.01ä½¿å…¶å€¼è¾ƒå°ï¼Œæœ‰åˆ©äºè®­ç»ƒç¨³å®šæ€§
            return torch.randn(size=shape, device=device)*0.01 # ç”Ÿæˆå°éšæœºæ•°

        def three(): # ç”Ÿæˆä¸é—¨æ§å’Œå€™é€‰éšçŠ¶æ€ç›¸å…³çš„ä¸‰ç»„å‚æ•°ï¼ˆè¾“å…¥æƒé‡ã€å¾ªç¯æƒé‡ã€åç½®ï¼‰
            """è¿”å›ä¸‰ä¸ªå‚æ•°ç»„ï¼š(è¾“å…¥æƒé‡, å¾ªç¯æƒé‡, åç½®)"""
            return (normal((num_inputs, num_hiddens)),       # è¾“å…¥åˆ°éšè—å±‚çš„æƒé‡
                    normal((num_hiddens, num_hiddens)),      # éšè—å±‚åˆ°éšè—å±‚çš„æƒé‡
                    torch.zeros(num_hiddens, device=device)) # åç½®ï¼ˆåˆå§‹åŒ–ä¸º0ï¼‰

        # ä¸ºGRUçš„ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼ˆæ›´æ–°é—¨ã€é‡ç½®é—¨ã€å€™é€‰çŠ¶æ€ï¼‰åˆ†åˆ«åˆ›å»ºå‚æ•°
        W_xz, W_hz, b_z = three()  # æ›´æ–°é—¨å‚æ•° (Zé—¨) : æ§åˆ¶æ–°æ—§çŠ¶æ€æ··åˆæ¯”ä¾‹
        W_xr, W_hr, b_r = three()  # é‡ç½®é—¨å‚æ•° (Ré—¨) : æ§åˆ¶å†å²ä¿¡æ¯é‡ç½®ç¨‹åº¦
        W_xh, W_hh, b_h = three()  # å€™é€‰éšçŠ¶æ€å‚æ•° : è®¡ç®—ä¸´æ—¶æ–°çŠ¶æ€

        # è¾“å‡ºå±‚å‚æ•°å•ç‹¬åˆå§‹åŒ– : å°†éšçŠ¶æ€æ˜ å°„åˆ°è¾“å‡ºç©ºé—´
        W_hq = normal((num_hiddens, num_outputs))       # éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡
        b_q = torch.zeros(num_outputs, device=device)   # è¾“å‡ºå±‚åç½®
        # å°†æ‰€æœ‰å‚æ•°æ”¾å…¥åˆ—è¡¨ï¼Œå¹¶é™„åŠ æ¢¯åº¦ï¼ˆç»„åˆæ‰€æœ‰å‚æ•°å¹¶å¯ç”¨æ¢¯åº¦è¿½è¸ªï¼‰
        params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]
        for param in params: # è®¾ç½®ä¸ºéœ€è¦æ¢¯åº¦
            param.requires_grad_(True)  # å¯ç”¨è‡ªåŠ¨å¾®åˆ†(å¼€å¯æ¢¯åº¦è¿½è¸ª)
        return params # è¿”å›æ¨¡å‹å‚æ•°åˆ—è¡¨

    # 2ã€å®šä¹‰è®­ç»ƒ
    '''
    åˆå§‹åŒ–GRUçš„éšè—çŠ¶æ€
    åŠŸèƒ½ï¼šè¿”å›åˆå§‹çš„éšè—çŠ¶æ€ï¼ˆä¸€ä¸ªå…¨é›¶å¼ é‡ï¼‰ï¼Œå½¢çŠ¶ä¸º (batch_size, num_hiddens)
    batch_size : æ‰¹é‡å¤§å°
    num_hiddens: éšè—å±‚å¤§å°
    device     : è®¡ç®—è®¾å¤‡
    è¿”å›:åŒ…å«åˆå§‹éšè—çŠ¶æ€çš„å…ƒç»„ï¼ˆå…¨é›¶å¼ é‡ï¼‰(H0,)
    è¯´æ˜ï¼šåœ¨è®­ç»ƒå¼€å§‹æ—¶æˆ–å¤„ç†æ–°åºåˆ—æ—¶ï¼ŒéšçŠ¶æ€éœ€è¦åˆå§‹åŒ–ä¸ºé›¶
    '''
    def init_gru_state(batch_size, num_hiddens, device):
        # åˆå§‹åŒ–éšçŠ¶æ€ä¸ºå…¨0ï¼Œå½¢çŠ¶ä¸º (batch_size, num_hiddenséšè—å•å…ƒæ•°)
        return (torch.zeros((batch_size, num_hiddens), device=device), ) # åˆå§‹éšè—çŠ¶æ€ä¸ºå…¨0

    '''
    GRUå‰å‘ä¼ æ’­è®¡ç®—
    æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºæ˜¯çº¿æ€§å˜æ¢åçš„éšçŠ¶æ€ï¼ˆæœªç»è¿‡softmaxï¼Œå› ä¸ºè®­ç»ƒæ—¶ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ä¼šåŒ…å«softmaxï¼‰
    inputs: è¾“å…¥åºåˆ— (æ—¶é—´æ­¥æ•°é‡, æ‰¹é‡å¤§å°, è¯è¡¨å¤§å°)
    state : åˆå§‹éšè—çŠ¶æ€çš„å…ƒç»„ (H0,)
    params: æ¨¡å‹å‚æ•°åˆ—è¡¨ [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]
    è¿”å›:
    outputs  : æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡ºï¼Œå½¢çŠ¶ä¸º (æ—¶é—´æ­¥æ•°é‡ * æ‰¹é‡å¤§å°, è¯è¡¨å¤§å°)
    (H,): æ›´æ–°åçš„æœ€ç»ˆéšçŠ¶æ€
    '''
    def gru(inputs, state, params):
        W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params # è§£åŒ…å‚æ•°
        H, = state       # è§£åŒ…å½“å‰éšè—çŠ¶æ€ (batch_size, num_hiddens)
        outputs = []     # ç”¨äºå­˜å‚¨æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        for X in inputs: # éå†è¾“å…¥åºåˆ—çš„æ¯ä¸ªæ—¶é—´æ­¥
            # æ›´æ–°é—¨è®¡ç®—ï¼šå†³å®šä¿ç•™å¤šå°‘æ—§çŠ¶æ€ (æ§åˆ¶çŠ¶æ€æ›´æ–°ç¨‹åº¦)
            # Z_t = Ïƒ(X_t * W_xz + H_{t-1} * W_hz + b_z)
            Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z) # å½¢çŠ¶: (batch_size, num_hiddens)

            # é‡ç½®é—¨è®¡ç®—ï¼šå†³å®šé‡ç½®å¤šå°‘å†å²ä¿¡æ¯
            # R_t = Ïƒ(X_t * W_xr + H_{t-1} * W_hr + b_r)
            R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r) # å½¢çŠ¶: (batch_size, num_hiddens)

            # å€™é€‰éšçŠ¶æ€è®¡ç®—ï¼šï¼ˆä½¿ç”¨é‡ç½®é—¨æ§åˆ¶å†å²ä¿¡æ¯å½±å“ï¼‰
            # \tilde{H}_t = tanh(X_t * W_xh + (R_t âŠ™ H_{t-1}) * W_hh + b_h)
            H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h) # å½¢çŠ¶: (batch_size, num_hiddens)

            # æ›´æ–°æœ€ç»ˆéšçŠ¶æ€ï¼šæ··åˆæ—§çŠ¶æ€å’Œæ–°å€™é€‰çŠ¶æ€ï¼šH_t = Z_t âŠ™ H_{t-1} + (1 - Z_t) âŠ™ \tilde{H}_t
            H = Z * H + (1 - Z) * H_tilda # å½¢çŠ¶: (batch_size, num_hiddens)

            # è®¡ç®—å½“å‰æ—¶é—´æ­¥è¾“å‡ºï¼Œå³ è¾“å‡ºå±‚ï¼šY_t = H_t * W_hq + b_q
            Y = H @ W_hq + b_q            # å½¢çŠ¶: (batch_size, vocab_size)
            outputs.append(Y)  # ä¿å­˜è¾“å‡º
        # æ²¿æ—¶é—´æ­¥ç»´åº¦æ‹¼æ¥æ‰€æœ‰è¾“å‡ºï¼ˆå½¢çŠ¶ï¼šæ—¶é—´æ­¥æ•°Ã—æ‰¹é‡å¤§å°Ã—è¯è¡¨å¤§å°ï¼‰
        # (H,) è¡¨ç¤ºåªåŒ…å«ä¸€ä¸ªå…ƒç´ Hçš„å…ƒç»„ï¼ˆtupleï¼‰ï¼Œä½†çœŸæ­£å®šä¹‰å…ƒç»„çš„æ˜¯é€—å·ï¼Œè€Œä¸æ˜¯åœ†æ‹¬å·
        # å¦‚æœå†™(H)ï¼Œè¿™åªæ˜¯å¸¦æ‹¬å·çš„è¡¨è¾¾å¼ï¼Œç­‰åŒäº`H`ï¼Œè€Œä¸æ˜¯å…ƒç»„ã€‚ä¸ºäº†è¡¨ç¤ºè¿™æ˜¯åªæœ‰ä¸€ä¸ªå…ƒç´ çš„å…ƒç»„ï¼Œéœ€åœ¨å…ƒç´ ååŠ é€—å·ã€‚æ‰€ä»¥ï¼š
        # (H)  æ˜¯ä¸€ä¸ªè¡¨è¾¾å¼ï¼Œå…¶å€¼ä¸ºH
        # (H,) æ˜¯ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ä¸€ä¸ªå…ƒç´ H
        return torch.cat(outputs, dim=0), (H,) # è¿”å›è¾“å‡ºå’Œæœ€åä¸€ä¸ªéšè—çŠ¶æ€

    # 3ã€è®­ç»ƒä¸é¢„æµ‹
    # è¯è¡¨å¤§å°ã€éšè—å±‚å¤§å°256ã€è®¾å¤‡
    vocab_size, num_hiddens, device = len(vocab), 256, common.try_gpu()
    num_epochs, lr = 500, 1 # è®­ç»ƒå‘¨æœŸå³è¿­ä»£å‘¨æœŸä¸º500ï¼Œå³è®­ç»ƒ500è½®ï¼›å­¦ä¹ ç‡ä¸º1ï¼ˆè¾ƒé«˜å­¦ä¹ ç‡å› ä»0å®ç°ï¼‰
    model = RNNModelScratch(len(vocab), num_hiddens, device, get_params,
                                init_gru_state, gru) # åˆ›å»ºæ¨¡å‹å®ä¾‹
    common.train_ch8(model, train_iter, vocab, lr, num_epochs, device) # è®­ç»ƒæ¨¡å‹
# learn_gru_StartFromScratch()



# å®šä¹‰å®Œæ•´çš„RNNæ¨¡å‹ç±»
# rnn_layeråªåŒ…å«éšè—çš„å¾ªç¯å±‚ï¼Œå› æ­¤å¦å¤–è¿˜éœ€è¦åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„è¾“å‡ºå±‚
class RNNModel(nn.Module):
    """å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹"""
    def __init__(self, rnn_layer, vocab_size, **kwargs):
        super(RNNModel, self).__init__(**kwargs)
        self.rnn = rnn_layer                    # ä¼ å…¥çš„RNNå±‚
        self.vocab_size = vocab_size            # è¯è¡¨å¤§å°
        self.num_hiddens = self.rnn.hidden_size # ä»RNNå±‚è·å–éšè—å•å…ƒæ•°

        # åˆ¤æ–­RNNæ˜¯å¦ä¸ºåŒå‘ï¼šè‹¥RNNæ˜¯åŒå‘(ä¹‹åå°†ä»‹ç»)ï¼Œnum_directionsä¸º2ï¼Œå¦åˆ™ä¸º1
        if not self.rnn.bidirectional: # å•å‘RNN: num_directions = 1
            self.num_directions = 1
            # çº¿æ€§å±‚: å°†éšè—çŠ¶æ€æ˜ å°„åˆ°è¯è¡¨å¤§å°
            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
        else:                          # åŒå‘RNN: num_directions = 2
            self.num_directions = 2
            # å¯¹äºåŒå‘RNNï¼Œéœ€è¦å°†ä¸¤ä¸ªæ–¹å‘çš„éšè—çŠ¶æ€æ‹¼æ¥
            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)

    """ å‰å‘ä¼ æ’­å‡½æ•°
    inputs: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º(æ‰¹é‡å¤§å°, æ—¶é—´æ­¥æ•°)
    state: éšè—çŠ¶æ€
    """
    def forward(self, inputs, state):
        # è¾“å…¥è½¬ä¸ºone-hotç¼–ç 
        # inputs.T: è½¬ç½®ä¸º(æ—¶é—´æ­¥æ•°, æ‰¹é‡å¤§å°)
        # .long() ï¼šå°†å¼ é‡è½¬æ¢ä¸ºé•¿æ•´å‹ï¼ˆint64ï¼‰ï¼ŒF.one_hotè¦æ±‚ç´¢å¼•ä¸ºæ•´å‹ï¼Œè€Œ.long()ç¡®ä¿è¿™ä¸€ç‚¹
        # one-hotç¼–ç åå½¢çŠ¶: (æ—¶é—´æ­¥æ•°, æ‰¹é‡å¤§å°, è¯è¡¨å¤§å°)
        X = F.one_hot(inputs.T.long(), self.vocab_size)
        X = X.to(torch.float32)         # ç‹¬çƒ­å¼ é‡è½¬æ¢ä¸ºfloat32 (PyTorchçš„çº¿æ€§å±‚éœ€è¦æµ®ç‚¹è¾“å…¥)

        # Y: æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡ºï¼Œå½¢çŠ¶ä¸º(æ—¶é—´æ­¥æ•°, æ‰¹é‡å¤§å°, éšè—å•å…ƒæ•° * æ–¹å‘æ•°)
        # state: æ›´æ–°åçš„éšè—çŠ¶æ€
        Y, state = self.rnn(X, state)   # é€šè¿‡RNNå±‚

        # å…¨è¿æ¥å±‚é¦–å…ˆå°†Yçš„å½¢çŠ¶æ”¹ä¸º(æ—¶é—´æ­¥æ•°*æ‰¹é‡å¤§å°,éšè—å•å…ƒæ•°)
        # å³ é‡å¡‘Yçš„å½¢çŠ¶ä¸º (æ—¶é—´æ­¥æ•° * æ‰¹é‡å¤§å°, éšè—å•å…ƒæ•° * æ–¹å‘æ•°)
        # è¿™æ ·æ¯ä¸ªæ—¶é—´æ­¥çš„æ¯ä¸ªæ ·æœ¬éƒ½å¯ä»¥ç‹¬ç«‹å¤„ç†
        # å…¶è¾“å‡ºå½¢çŠ¶ä¸º (æ—¶é—´æ­¥æ•°*æ‰¹é‡å¤§å°, è¯è¡¨å¤§å°)
        output = self.linear(Y.reshape((-1, Y.shape[-1]))) # é€šè¿‡å…¨è¿æ¥å±‚è¿›è¡Œé¢„æµ‹
        return output, state

    # åˆå§‹åŒ–éšè—çŠ¶æ€å‡½æ•°ï¼šæ ¹æ®RNNç±»å‹(GRU/LSTM)è¿”å›é€‚å½“å½¢å¼çš„åˆå§‹çŠ¶æ€
    def begin_state(self, device, batch_size=1):
        if not isinstance(self.rnn, nn.LSTM): # å¯¹äºGRUç±»å‹çš„RNN
            # nn.GRUä»¥å¼ é‡ä½œä¸ºéšçŠ¶æ€
            # è¿”å›é›¶å¼ é‡ä½œä¸ºåˆå§‹çŠ¶æ€ï¼Œå½¢çŠ¶: (å±‚æ•° * æ–¹å‘æ•°, æ‰¹é‡å¤§å°, éšè—å•å…ƒæ•°)
            return  torch.zeros((self.num_directions * self.rnn.num_layers,
                                 batch_size, self.num_hiddens),
                                device=device)
        else: # å¯¹äºLSTMç±»å‹çš„RNN
            # nn.LSTMä»¥å…ƒç»„ä½œä¸ºéšçŠ¶æ€ï¼ŒLSTMéœ€è¦ä¸¤ä¸ªçŠ¶æ€: (éšè—çŠ¶æ€h, ç»†èƒçŠ¶æ€c)
            return (
                torch.zeros(( # éšè—çŠ¶æ€
                self.num_directions * self.rnn.num_layers,
                batch_size, self.num_hiddens), device=device),
                torch.zeros(( # ç»†èƒçŠ¶æ€
                        self.num_directions * self.rnn.num_layers,
                        batch_size, self.num_hiddens), device=device))


# é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰çš„ç®€æ´å®ç°
def learn_gru_SimpleImplementation():
    # è¯è¡¨å¤§å°ã€éšè—å±‚å¤§å°256ã€è®¾å¤‡
    vocab_size, num_hiddens, device = len(vocab), 256, common.try_gpu()
    num_epochs, lr = 500, 1 # è®­ç»ƒå‘¨æœŸå³è¿­ä»£å‘¨æœŸä¸º500ï¼Œå³è®­ç»ƒ500è½®ï¼›å­¦ä¹ ç‡ä¸º1(ä½¿ç”¨å†…ç½®GRUæ—¶é€šå¸¸éœ€è¦è°ƒæ•´ï¼Œè¿™é‡Œä¿æŒä¸ä»é›¶å®ç°ä¸€è‡´)

    num_inputs = vocab_size # è¾“å…¥ç»´åº¦ç­‰äºè¯è¡¨å¤§å°ï¼ˆå­—ç¬¦çº§one-hotè¡¨ç¤ºï¼‰
    """
    nn.GRUå…³é”®å‚æ•°:
    - num_inputs: è¾“å…¥ç‰¹å¾ç»´åº¦ (è¯è¡¨å¤§å°)
    - num_hiddens: éšè—å±‚ç¥ç»å…ƒæ•°é‡ (256)
    - é»˜è®¤: å•å±‚ã€éåŒå‘ã€batch_first=False (åºåˆ—ç»´åº¦åœ¨å‰)
    """
    gru_layer = nn.GRU(num_inputs, num_hiddens) # åˆ›å»ºå†…ç½®GRUå±‚ï¼šè¾“å…¥ç»´åº¦, éšè—å±‚ç»´åº¦
    """
    RNNModelç±»åŠŸèƒ½:
    1. å¤„ç†è¾“å…¥æ•°æ®çš„one-hotç¼–ç 
    2. é€šè¿‡GRUå±‚è®¡ç®—éšè—çŠ¶æ€
    3. æ·»åŠ å…¨è¿æ¥è¾“å‡ºå±‚ (éšè—å±‚->è¯è¡¨)
    4. ç®¡ç†éšè—çŠ¶æ€çš„åˆå§‹åŒ–å’Œä¼ é€’
    """
    model = RNNModel(gru_layer, len(vocab)) # ä¼ å…¥GRUå±‚å’Œè¯è¡¨å¤§å°
    model = model.to(device) # å°†æ¨¡å‹ç§»åˆ°æŒ‡å®šè®¾å¤‡ (GPU/CPU)
    common.train_ch8(model, train_iter, vocab, lr, num_epochs, device) # è®­ç»ƒæ¨¡å‹
# learn_gru_SimpleImplementation()



# é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰çš„ä»é›¶å¼€å§‹å®ç°
def learn_LSTM_StartFromScratch():
    # 1ã€åˆå§‹åŒ–æ¨¡å‹å‚æ•°
    """
    åˆå§‹åŒ– é•¿çŸ­æœŸè®°å¿†ç½‘ç»œLSTM æ‰€æœ‰å¯è®­ç»ƒå‚æ•°
    æƒé‡ä½¿ç”¨å°éšæœºæ•°åˆå§‹åŒ–ï¼Œåç½®åˆå§‹åŒ–ä¸º0
    å‚æ•°:
    vocab_size : è¯è¡¨å¤§å° (è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦)
    num_hiddens: éšè—å±‚æ•°é‡
    device     : è®¡ç®—è®¾å¤‡ (CPU/GPU)
    è¿”å›:
    params: åŒ…å«æ‰€æœ‰å‚æ•°çš„åˆ—è¡¨
    """
    def get_lstm_params(vocab_size, num_hiddens, device):
        num_inputs = num_outputs = vocab_size # è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦éƒ½æ˜¯è¯è¡¨å¤§å°ï¼ˆå­—ç¬¦çº§è¯­è¨€æ¨¡å‹ï¼‰

        def normal(shape): # å®šä¹‰æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–å‡½æ•°
            """ç”Ÿæˆæœä»æ­£æ€åˆ†å¸ƒçš„éšæœºå¼ é‡ï¼ˆç¼©å°åˆå§‹å€¼èŒƒå›´ï¼‰"""
            # ç”Ÿæˆæœä»æ­£æ€åˆ†å¸ƒçš„éšæœºå‚æ•°ï¼Œä¹˜ä»¥0.01ä½¿å…¶å€¼è¾ƒå°ï¼Œæœ‰åˆ©äºè®­ç»ƒç¨³å®šæ€§
            return torch.randn(size=shape, device=device)*0.01 # ç”Ÿæˆå°éšæœºæ•°

        def three():
            """è¿”å›ä¸‰ä¸ªå‚æ•°ç»„ï¼š(è¾“å…¥æƒé‡, å¾ªç¯æƒé‡, åç½®)"""
            return (normal((num_inputs, num_hiddens)),        # è¾“å…¥åˆ°éšè—å±‚çš„æƒé‡
                    normal((num_hiddens, num_hiddens)),       # éšè—å±‚åˆ°éšè—å±‚çš„æƒé‡
                    torch.zeros(num_hiddens, device=device))  # åç½®ï¼ˆåˆå§‹åŒ–ä¸º0ï¼‰

        W_xi, W_hi, b_i = three()  # è¾“å…¥é—¨å‚æ•° (æ§åˆ¶æ–°ä¿¡æ¯æµå…¥)
        W_xf, W_hf, b_f = three()  # é—å¿˜é—¨å‚æ•° (æ§åˆ¶æ—§ä¿¡æ¯ä¿ç•™)
        W_xo, W_ho, b_o = three()  # è¾“å‡ºé—¨å‚æ•° (æ§åˆ¶ä¿¡æ¯è¾“å‡º)
        W_xc, W_hc, b_c = three()  # å€™é€‰è®°å¿†å…ƒå‚æ•° (æ–°è®°å¿†è®¡ç®—)
        # è¾“å‡ºå±‚å‚æ•°å•ç‹¬åˆå§‹åŒ– : å°†éšçŠ¶æ€æ˜ å°„åˆ°è¾“å‡ºç©ºé—´ï¼ˆè¯è¡¨å¤§å°ï¼‰
        W_hq = normal((num_hiddens, num_outputs))      # éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡
        b_q = torch.zeros(num_outputs, device=device)  # è¾“å‡ºå±‚åç½®ï¼ˆåˆå§‹åŒ–ä¸º0ï¼‰
        # å°†æ‰€æœ‰å‚æ•°æ”¾å…¥åˆ—è¡¨ï¼Œå¹¶é™„åŠ æ¢¯åº¦ï¼ˆç»„åˆæ‰€æœ‰å‚æ•°å¹¶å¯ç”¨æ¢¯åº¦è¿½è¸ªï¼‰
        params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,
                  b_c, W_hq, b_q]
        for param in params:  # è®¾ç½®ä¸ºéœ€è¦æ¢¯åº¦
            param.requires_grad_(True)  # å¯ç”¨è‡ªåŠ¨å¾®åˆ†(å¼€å¯æ¢¯åº¦è¿½è¸ª)
        return params # è¿”å›æ¨¡å‹å‚æ•°åˆ—è¡¨

    # 2ã€å®šä¹‰æ¨¡å‹
    '''
    åˆå§‹åŒ– LSTMçš„ éšè—çŠ¶æ€ å’Œ è®°å¿†å…ƒçŠ¶æ€
    batch_size : æ‰¹é‡å¤§å°
    num_hiddens: éšè—å±‚å¤§å°
    device     : è®¡ç®—è®¾å¤‡
    è¿”å›:åŒ…å«(åˆå§‹éšè—çŠ¶æ€, åˆå§‹è®°å¿†å…ƒçŠ¶æ€)çš„å…ƒç»„ï¼ˆå…¨é›¶å¼ é‡ï¼‰
    '''
    def init_lstm_state(batch_size, num_hiddens, device):
        # LSTMæœ‰ä¸¤ä¸ªçŠ¶æ€ï¼šéšè—çŠ¶æ€Hå’Œè®°å¿†å…ƒçŠ¶æ€Cï¼Œåˆå§‹åŒ–ä¸ºå…¨0ï¼Œå½¢çŠ¶ä¸º (batch_size, num_hiddenséšè—å•å…ƒæ•°)
        return (torch.zeros((batch_size, num_hiddens), device=device), # éšè—çŠ¶æ€ H
                torch.zeros((batch_size, num_hiddens), device=device)) # è®°å¿†å…ƒçŠ¶æ€ C

    '''
    LSTMå‰å‘ä¼ æ’­è®¡ç®—
    æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºæ˜¯çº¿æ€§å˜æ¢åçš„éšçŠ¶æ€ï¼ˆæœªç»è¿‡softmaxï¼Œå› ä¸ºè®­ç»ƒæ—¶ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ä¼šåŒ…å«softmaxï¼‰
    inputs: è¾“å…¥åºåˆ— (æ—¶é—´æ­¥åˆ—è¡¨, æ¯ä¸ªå½¢çŠ¶ä¸º[batch_size, vocab_size]) å³ (æ—¶é—´æ­¥æ•°é‡, æ‰¹é‡å¤§å°, è¯è¡¨å¤§å°)
    state : åˆå§‹éšè—çŠ¶æ€çš„å…ƒç»„ (H, C)
    params: æ¨¡å‹å‚æ•°åˆ—è¡¨
    è¿”å›:
    outputs  : æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡ºï¼Œå½¢çŠ¶ä¸º (æ—¶é—´æ­¥æ•°é‡ * æ‰¹é‡å¤§å°, è¯è¡¨å¤§å°)
    (H, C): æ›´æ–°åçš„æœ€ç»ˆçŠ¶æ€
    '''
    def lstm(inputs, state, params):
        [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,
         W_hq, b_q] = params # è§£åŒ…å‚æ•° (å…±14ä¸ªå‚æ•°)
        (H, C) = state       # è§£åŒ…åˆå§‹çŠ¶æ€ï¼ˆH: éšè—çŠ¶æ€, C: è®°å¿†å…ƒçŠ¶æ€ï¼‰
        outputs = []         # ç”¨äºå­˜å‚¨æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        for X in inputs:     # éå†è¾“å…¥åºåˆ—çš„æ¯ä¸ªæ—¶é—´æ­¥
            # 1. è¾“å…¥é—¨ (I_t)è®¡ç®—ï¼šæ§åˆ¶æ–°ä¿¡æ¯æµå…¥
            I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i) # å½¢çŠ¶: (batch_size, num_hiddens)

            # 2. é—å¿˜é—¨ (F_t)è®¡ç®—ï¼šæ§åˆ¶æ—§ä¿¡æ¯ä¿ç•™
            F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f) # å½¢çŠ¶: (batch_size, num_hiddens)

            # 3. è¾“å‡ºé—¨ (O_t)è®¡ç®—ï¼šæ§åˆ¶ä¿¡æ¯è¾“å‡º
            O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o) # å½¢çŠ¶: (batch_size, num_hiddens)

            # 4. å€™é€‰è®°å¿†å…ƒ (C_tilda_t)è®¡ç®—ï¼šæ–°ä¿¡æ¯çš„åŸå§‹è¡¨ç¤º
            C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c) # å½¢çŠ¶: (batch_size, num_hiddens)

            # 5. æ›´æ–°è®°å¿†å…ƒ (C_t)çŠ¶æ€ï¼šé—å¿˜æ—§ä¿¡æ¯ + æ·»åŠ æ–°ä¿¡æ¯ ï¼ˆé—å¿˜é—¨æ§åˆ¶æ—§çŠ¶æ€ï¼Œè¾“å…¥é—¨æ§åˆ¶æ–°å€™é€‰çŠ¶æ€ï¼‰
            C = F * C + I * C_tilda # å½¢çŠ¶: (batch_size, num_hiddens)

            # 6. æ›´æ–°éšè—çŠ¶æ€ (H_t)ï¼šåŸºäºè®°å¿†å…ƒç”Ÿæˆæ–°éšè—çŠ¶æ€ï¼ˆè¾“å‡ºé—¨æ§åˆ¶tanh(è®°å¿†å…ƒ)çš„è¾“å‡ºï¼‰
            H = O * torch.tanh(C) # å½¢çŠ¶: (batch_size, num_hiddens)

            # 7. è®¡ç®—å½“å‰æ—¶é—´æ­¥è¾“å‡º (Y_t)
            Y = (H @ W_hq) + b_q  # å½¢çŠ¶: (batch_size, vocab_size)
            outputs.append(Y)     # ä¿å­˜è¾“å‡º
        return torch.cat(outputs, dim=0), (H, C) # æ²¿æ—¶é—´æ­¥ç»´åº¦æ‹¼æ¥æ‰€æœ‰è¾“å‡ºï¼ˆå½¢çŠ¶ï¼šæ—¶é—´æ­¥æ•°Ã—æ‰¹é‡å¤§å°Ã—è¯è¡¨å¤§å°ï¼‰

    # 3ã€è®­ç»ƒå’Œé¢„æµ‹
    # è¯è¡¨å¤§å°ã€éšè—å±‚å¤§å°256ã€è®¾å¤‡
    vocab_size, num_hiddens, device = len(vocab), 256, common.try_gpu()
    num_epochs, lr = 500, 1 # è®­ç»ƒå‘¨æœŸå³è¿­ä»£å‘¨æœŸä¸º500ï¼Œå³è®­ç»ƒ500è½®ï¼›å­¦ä¹ ç‡ä¸º1ï¼ˆè¾ƒé«˜å­¦ä¹ ç‡å› ä»é›¶å®ç°ï¼‰
    # åˆ›å»ºLSTMæ¨¡å‹å®ä¾‹ï¼ˆä½¿ç”¨RNNModelScratchç±»å°è£…ï¼Œä¼ å…¥LSTMçš„å‚æ•°åˆå§‹åŒ–ã€çŠ¶æ€åˆå§‹åŒ–å’Œå‰å‘ä¼ æ’­å‡½æ•°ï¼‰
    model = RNNModelScratch(len(vocab), num_hiddens, device, get_lstm_params,
                                init_lstm_state, lstm)
    # è®­ç»ƒæ¨¡å‹ï¼ˆtrain_iteræ˜¯æ•°æ®è¿­ä»£å™¨ï¼Œvocabæ˜¯è¯è¡¨ï¼‰
    common.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
# learn_LSTM_StartFromScratch()


# é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰çš„ç®€æ´å®ç°
def learn_LSTM_SimpleImplementation():
    # è¯è¡¨å¤§å°ï¼ˆå”¯ä¸€å­—ç¬¦æ•°é‡ï¼‰ã€LSTMéšè—å±‚ç»´åº¦ï¼ˆ256ä¸ªç¥ç»å…ƒï¼‰ã€è®¾å¤‡
    vocab_size, num_hiddens, device = len(vocab), 256, common.try_gpu()
    # è®­ç»ƒå‘¨æœŸå³è¿­ä»£å‘¨æœŸä¸º500ï¼Œå³è®­ç»ƒ500è½®ï¼›
    # å­¦ä¹ ç‡ä¸º1(ä½¿ç”¨å†…ç½®GRUæ—¶é€šå¸¸éœ€è¦è°ƒæ•´ï¼Œè¿™é‡Œä¿æŒä¸ä»é›¶å®ç°ä¸€è‡´ï¼Œå®é™…ä½¿ç”¨å†…ç½®LSTMæ—¶å»ºè®®0.01-0.1)
    num_epochs, lr = 500, 1
    num_inputs = vocab_size # è¾“å…¥ç»´åº¦ç­‰äºè¯è¡¨å¤§å°ï¼ˆå­—ç¬¦çº§one-hotè¡¨ç¤ºï¼‰
    """
    nn.LSTMå…³é”®å‚æ•°:
    - input_size: è¾“å…¥ç‰¹å¾ç»´åº¦ = è¯è¡¨å¤§å°
    - hidden_size: éšè—çŠ¶æ€ç»´åº¦ = 256
    - é»˜è®¤é…ç½®:
      - å•å±‚å•å‘LSTM
      - batch_first=False (è¾“å…¥å½¢çŠ¶ä¸º[seq_len, batch, input_size])
      - ä½¿ç”¨tanhæ¿€æ´»å‡½æ•°
      - åç½®é¡¹é»˜è®¤å¯ç”¨
    """
    lstm_layer = nn.LSTM(num_inputs, num_hiddens)
    model = RNNModel(lstm_layer, len(vocab)) # ä¼ å…¥GRUå±‚å’Œè¯è¡¨å¤§å°
    """
    RNNModelç±»å°è£…äº†:
    1. è¾“å…¥å¤„ç†: å°†å­—ç¬¦ç´¢å¼•è½¬æ¢ä¸ºå‘é‡ï¼ˆå¯èƒ½ä½¿ç”¨åµŒå…¥å±‚æˆ–one-hotï¼‰
    2. LSTMå±‚: æ ¸å¿ƒåºåˆ—å»ºæ¨¡
    3. è¾“å‡ºå±‚: å…¨è¿æ¥å±‚ (256éšè—å•å…ƒ â†’ è¯è¡¨å¤§å°)
    4. çŠ¶æ€ç®¡ç†: è‡ªåŠ¨å¤„ç†LSTMçš„éšè—çŠ¶æ€å’Œè®°å¿†å…ƒçŠ¶æ€
    
    é¢„æœŸç»“æ„:
    class RNNModel(nn.Module):
        def __init__(self, rnn_layer, vocab_size):
            super().__init__()
            self.rnn = rnn_layer
            self.vocab_size = vocab_size
            self.num_hiddens = rnn_layer.hidden_size
            # è¾“å‡ºå±‚
            self.dense = nn.Linear(self.num_hiddens, vocab_size)
    
        def forward(self, inputs, state):
            # è¾“å…¥è½¬æ¢ (batch, seq_len) â†’ (seq_len, batch, vocab_size)
            X = F.one_hot(inputs.T.long(), self.vocab_size).float()
            # LSTMè®¡ç®—
            Y, state = self.rnn(X, state)
            # è¾“å‡ºå±‚
            output = self.dense(Y.reshape(-1, Y.shape[-1]))
            return output, state
    
        def begin_state(self, batch_size=1):
            return (torch.zeros(1, batch_size, self.num_hiddens),
                    torch.zeros(1, batch_size, self.num_hiddens))
    """

    model = model.to(device) # å°†æ¨¡å‹ç§»åˆ°æŒ‡å®šè®¾å¤‡ (GPU/CPU)
    common.train_ch8(model, train_iter, vocab, lr, num_epochs, device) # è®­ç»ƒæ¨¡å‹
# learn_LSTM_SimpleImplementation()


# æ·±åº¦å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆä»…ç¤ºèŒƒç®€æ´å®ç°ï¼‰
def learn_drnn():
    # è¯è¡¨å¤§å°ï¼Œéšè—å±‚æ•°é‡
    # num_layers : LSTMå±‚æ•° = 2 (å †å ä¸¤å±‚LSTM)
    vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
    num_inputs = vocab_size     # è¾“å…¥ç‰¹å¾ç»´åº¦ä¸ºè¯è¡¨å¤§å°ï¼ˆæ¯ä¸ªå­—ç¬¦ç”¨one-hotè¡¨ç¤ºï¼‰
    device = common.try_gpu()   # è®¾å¤‡
    # input_size : è¾“å…¥ç‰¹å¾ç»´åº¦ = è¯è¡¨å¤§å°
    # hidden_size: éšè—çŠ¶æ€ç»´åº¦ = 256
    # num_layers : LSTMå±‚æ•° = 2 (å †å ä¸¤å±‚LSTM)
    lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers) # åˆ›å»ºLSTMå±‚
    model = RNNModel(lstm_layer, len(vocab)) # åˆ›å»ºå®Œæ•´çš„RNNæ¨¡å‹
    model = model.to(device) # å°†æ¨¡å‹ç§»è‡³æŒ‡å®šè®¾å¤‡

    num_epochs, lr = 500, 2  # è¿­ä»£è½®æ•°ä¸º500ï¼Œå­¦ä¹ ç‡
    common.train_ch8(model, train_iter, vocab, lr*1.0, num_epochs, device)
# learn_drnn()


# åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ çš„é”™è¯¯ç¤ºèŒƒ
def bidirectionalRNN_incorrect_demonstration():
    # è¯è¡¨å¤§å°ï¼Œéšè—å±‚æ•°é‡ï¼ŒLSTMå±‚æ•°ä¸º2 (å †å ä¸¤å±‚LSTM)
    vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
    num_inputs = vocab_size     # è¾“å…¥ç‰¹å¾ç»´åº¦ä¸ºè¯è¡¨å¤§å°ï¼ˆæ¯ä¸ªå­—ç¬¦ç”¨one-hotè¡¨ç¤ºï¼‰
    device = common.try_gpu()   # è®¾å¤‡
    # é€šè¿‡è®¾ç½®â€œbidirective=Trueâ€å¯ç”¨åŒå‘ æ¥å®šä¹‰åŒå‘LSTMæ¨¡å‹
    # ä½¿ç”¨åŒå‘LSTMæ¥æ„å»ºè¯­è¨€æ¨¡å‹
    lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=True)
    '''
    nn.LSTMå‚æ•°è¯´æ˜:
    - input_size: è¾“å…¥ç‰¹å¾ç»´åº¦ = vocab_size
    - hidden_size: éšè—çŠ¶æ€ç»´åº¦ = 256
    - num_layers: LSTMå±‚æ•° = 2 (å †å ä¸¤å±‚)
    - bidirectional=True: å¯ç”¨åŒå‘å¤„ç†
    
    åŒå‘LSTMçš„ç‰¹ç‚¹:
    1. æ¯ä¸ªæ—¶é—´æ­¥åŒ…å«ä¸¤ä¸ªéšè—çŠ¶æ€:
       - å‰å‘çŠ¶æ€: å¤„ç†è¿‡å»â†’æœªæ¥çš„ä¿¡æ¯
       - åå‘çŠ¶æ€: å¤„ç†æœªæ¥â†’è¿‡å»çš„ä¿¡æ¯
    2. è¾“å‡ºç»´åº¦å˜åŒ–:
       - å•å±‚åŒå‘: éšè—çŠ¶æ€ç»´åº¦ = 2 * num_hiddens
       - å¤šå±‚åŒå‘: è¾“å‡ºå½¢çŠ¶ = (seq_len, batch, 2*num_hiddens)
    '''
    model = RNNModel(lstm_layer, len(vocab)) # åˆ›å»ºå®Œæ•´çš„RNNæ¨¡å‹
    model = model.to(device) # å°†æ¨¡å‹ç§»è‡³æŒ‡å®šè®¾å¤‡
    # è®­ç»ƒæ¨¡å‹
    num_epochs, lr = 500, 1  # è¿­ä»£è½®æ•°ä¸º500ï¼Œå­¦ä¹ ç‡
    common.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
# bidirectionalRNN_incorrect_demonstration()


# æ³¨å†Œæ•°æ®é›†ä¿¡æ¯åˆ°DATA_HUBå…¨å±€å­—å…¸
# æ ¼å¼ï¼š(æ•°æ®é›†URL, MD5æ ¡éªŒå€¼)
DATA_HUB['fra-eng'] = (DATA_URL + 'fra-eng.zip', # å®Œæ•´ä¸‹è½½URLï¼ˆDATA_URLæ˜¯d2lå®šä¹‰çš„åŸºå‡†URLï¼‰
                           '94646ad1522d915e7b0f9296181140edcf86a4f5') # æ–‡ä»¶MD5ï¼Œç”¨äºæ ¡éªŒä¸‹è½½å®Œæ•´æ€§

def learn_MachinaTranslation_and_Data():
    # è½½å…¥(ç¥ç»ç½‘ç»œæœºå™¨ç¿»è¯‘nmtä¸­è¦ç”¨åˆ°çš„)â€œè‹±è¯­ï¼æ³•è¯­â€æ•°æ®é›†
    raw_text = common.read_data_nmt(downloader) # æ­¤æ—¶raw_textåŒ…å«æ•´ä¸ªè¯­æ–™åº“çš„åŸå§‹æ–‡æœ¬
    print(f"æ•°æ®é›†é¢„è§ˆï¼ˆæ˜¾ç¤ºå‰75ä¸ªå­—ç¬¦ï¼‰ï¼š\n{raw_text[:75]}")

    text = common.preprocess_nmt(raw_text)
    print(f"é¢„å¤„ç†å æ•°æ®é›†é¢„è§ˆï¼ˆæ˜¾ç¤ºå‰80ä¸ªå­—ç¬¦ï¼‰ï¼š\n{text[:80]}")

    source, target = common.tokenize_nmt(text)
    print(f"æºè¯­è¨€(è‹±è¯­)ã€å‰6ä¸ªã€‘æ–‡æœ¬åºåˆ—çš„è¯å…ƒåˆ—è¡¨ï¼š\n{source[:6]}")
    print(f"ç›®æ ‡è¯­è¨€(æ³•è¯­)ã€å‰6ä¸ªã€‘æ–‡æœ¬åºåˆ—çš„è¯å…ƒåˆ—è¡¨ï¼š\n{target[:6]}")

    # ç»˜åˆ¶æ¯ä¸ªæ–‡æœ¬åºåˆ—æ‰€åŒ…å«çš„è¯å…ƒæ•°é‡çš„ç›´æ–¹å›¾
    # common.show_list_len_pair_hist(['source', 'target'],
    #                                '# tokens per sequence','count',
    #                                source, target)

    # ç»™ è¯å…ƒåŒ–åçš„æºè¯­è¨€ å¥å­åˆ—è¡¨ æ„å»ºè¯è¡¨ï¼Œç®¡ç†è¯å…ƒä¸ç´¢å¼•çš„æ˜ å°„å…³ç³»
    src_vocab = common.Vocab(source, min_freq=2,
                          reserved_tokens=['<pad>', '<bos>', '<eos>'])
    print(f"è¯è¡¨å¤§å°ï¼š{len(src_vocab)}")

    # æ‰“å°å‡ºå…ƒç´ ä¸ºæ•´å‹æ˜¯å› ä¸º src_vocabå·²å°†å•è¯æ˜ å°„ä¸ºæ•´æ•°ID
    print(f"ä»¥å–ç¬¬ä¸€ä¸ªå¥å­ä¸ºä¾‹\n{src_vocab[source[0]]}")
    print(f"å°†æ–‡æœ¬åºåˆ—æˆªæ–­æˆ–å¡«å……è‡³10ä¸ªåï¼š(å–ç¬¬ä¸€ä¸ªå¥å­ä¸ºä¾‹)\n"
          f"{common.truncate_pad(src_vocab[source[0]], 10, src_vocab['<pad>'])}")

    # åŠ è½½â€œè‹±è¯­ï¼æ³•è¯­â€æ•°æ®é›†çš„æ•°æ®è¿­ä»£å™¨ï¼Œä»¥åŠæºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„è¯è¡¨
    train_iter, src_vocab, tgt_vocab = common.load_data_nmt(downloader, batch_size=2, num_steps=8)
    # è¯»å‡ºâ€œè‹±è¯­ï¼æ³•è¯­â€æ•°æ®é›†ä¸­çš„ç¬¬ä¸€ä¸ªå°æ‰¹é‡æ•°æ®
    for X, X_valid_len, Y, Y_valid_len in train_iter:
        print('æºè¯­è¨€X:', X.type(torch.int32))
        print('æºè¯­è¨€Xçš„æœ‰æ•ˆé•¿åº¦:', X_valid_len)
        print('ç›®æ ‡è¯­è¨€Y:', Y.type(torch.int32))
        print('ç›®æ ‡è¯­è¨€Yçš„æœ‰æ•ˆé•¿åº¦:', Y_valid_len)
        break
learn_MachinaTranslation_and_Data()




# ç”¨äºåºåˆ—åˆ°åºåˆ—å­¦ä¹ ï¼ˆseq2seqï¼‰çš„å¾ªç¯ç¥ç»ç½‘ç»œç¼–ç å™¨
class Seq2SeqEncoder(common.Encoder):
    """ç”¨äºåºåˆ—åˆ°åºåˆ—å­¦ä¹ çš„å¾ªç¯ç¥ç»ç½‘ç»œç¼–ç å™¨"""
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqEncoder, self).__init__(**kwargs)
        # åµŒå…¥å±‚:è·å¾—è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªè¯å…ƒçš„ç‰¹å¾å‘é‡
        # å°†è¯å…ƒç´¢å¼•è½¬æ¢ä¸ºå¯†é›†å‘é‡ï¼ˆvocab_size â†’ embed_sizeï¼‰
        self.embedding = nn.Embedding(vocab_size, embed_size)

        # å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆGRUï¼‰
        # embed_size: è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆåµŒå…¥å±‚è¾“å‡ºç»´åº¦ï¼‰
        # num_hiddens: éšçŠ¶æ€ç»´åº¦
        # num_layers: å †å çš„RNNå±‚æ•°
        # dropout: å±‚é—´dropoutæ¦‚ç‡ï¼ˆä»…åœ¨num_layers>1æ—¶ç”Ÿæ•ˆ
        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,
                          dropout=dropout)

    def forward(self, X, *args): # å‰å‘ä¼ æ’­é€»è¾‘
        # è¾“å‡º'X'çš„å½¢çŠ¶ï¼š(batch_size,num_steps,embed_size)
        X = self.embedding(X) # åµŒå…¥å±‚å¤„ç†ï¼š(bch_sz, num_steps)â†’ (bch_sz, num_steps, embed_size)

        # permute(1, 0, 2)äº¤æ¢å‰ä¸¤ç»´ï¼Œå³ æ—¶é—´æ­¥å’Œæ‰¹æ¬¡ç»´åº¦
        # å› ä¸ºåœ¨å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­ï¼Œè½´ä¸€å¯¹åº”äºæ—¶é—´æ­¥
        # å³ RNNè¦æ±‚è¾“å…¥å½¢çŠ¶ä¸º (num_steps, batch_size, embed_size)
        X = X.permute(1, 0, 2) # å½¢çŠ¶å˜ä¸º (num_steps, batch_size, embed_size)

        # RNNå¤„ç†
        # é»˜è®¤åˆå§‹éšçŠ¶æ€state=Noneæ—¶ï¼Œè‡ªåŠ¨åˆå§‹åŒ–ä¸ºå…¨é›¶
        # è‹¥æœªæåŠçŠ¶æ€ï¼Œåˆ™é»˜è®¤ä¸º0
        output, state = self.rnn(X)

        # outputçš„å½¢çŠ¶:(num_steps,batch_size,num_hiddens)
        # stateçš„å½¢çŠ¶:(num_layers,batch_size,num_hiddens)
        return output, state # æ‰€æœ‰æ—¶é—´æ­¥çš„éšçŠ¶æ€ï¼Œæœ€åä¸€å±‚çš„æœ€ç»ˆéšçŠ¶æ€

encoder = Seq2SeqEncoder(vocab_size=10,  # è¯è¡¨å¤§å° å³ è¾“å…¥ç»´åº¦ä¸º10
                         embed_size=8,   # æ¯ä¸ªå•è¯è¢«è¡¨ç¤ºä¸º8ç»´çš„å‘é‡ï¼Œå³ ç‰¹å¾å‘é‡çš„ç»´åº¦
                         num_hiddens=16, # éšè—å±‚çš„ç»´åº¦ å³ å•ä¸ªéšè—å±‚çš„ç¥ç»å…ƒæ•°é‡
                         num_layers=2)   # éšè—å±‚çš„å †å æ¬¡æ•°ä¸º2
encoder.eval() # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­dropoutç­‰è®­ç»ƒä¸“ç”¨å±‚ï¼‰
# æ¨¡æ‹Ÿè¾“å…¥ï¼š4ä¸ªåºåˆ—ï¼ˆbatch_size=4ï¼‰ï¼Œæ¯ä¸ªåºåˆ—7ä¸ªæ—¶é—´æ­¥ï¼ˆnum_steps=7ï¼‰
X = torch.zeros((4, 7), dtype=torch.long)
output, state = encoder(X) # å‰å‘ä¼ æ’­
print(f"æœ€åä¸€å±‚çš„éšçŠ¶æ€çš„è¾“å‡º(æ‰€æœ‰æ—¶é—´æ­¥)ï¼š{output.shape}")
print(f"æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„å¤šå±‚éšçŠ¶æ€çš„å½¢çŠ¶ï¼š{state.shape}")


# ç”¨äºåºåˆ—åˆ°åºåˆ—å­¦ä¹ ï¼ˆseq2seqï¼‰çš„å¾ªç¯ç¥ç»ç½‘ç»œè§£ç å™¨
class Seq2SeqDecoder(common.Decoder):
    """ç”¨äºåºåˆ—åˆ°åºåˆ—å­¦ä¹ çš„å¾ªç¯ç¥ç»ç½‘ç»œè§£ç å™¨"""
    """ åˆå§‹åŒ–è§£ç å™¨
    vocab_size : ç›®æ ‡è¯­è¨€è¯è¡¨å¤§å°
    embed_size : è¯åµŒå…¥ç»´åº¦
    num_hiddens: GRUéšè—å±‚å¤§å°ï¼Œå³ éšè—å±‚ç»´åº¦
    num_layers : GRUå±‚æ•°
    dropout    : éšæœºå¤±æ´»ç‡ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    """
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqDecoder, self).__init__(**kwargs)
        # è¯åµŒå…¥å±‚ï¼šå°†è¯æ±‡è¡¨ç´¢å¼•è½¬æ¢ä¸ºç¨ å¯†å‘é‡
        self.embedding = nn.Embedding(vocab_size, embed_size)
        # GRUè§£ç å™¨æ ¸å¿ƒï¼š
        # è¾“å…¥ç»´åº¦ = embed_sizeï¼ˆå½“å‰è¾“å…¥ï¼‰ + num_hiddensï¼ˆä¸Šä¸‹æ–‡å˜é‡ï¼‰
        # ä¸Šä¸‹æ–‡å‘é‡ï¼šæ¥è‡ªç¼–ç å™¨çš„æœ€ç»ˆéšè—çŠ¶æ€(è¿™ç§è®¾è®¡å…è®¸è§£ç å™¨è®¿é—®æºåºåˆ—ä¿¡æ¯)
        # éšè—å±‚ç»´åº¦ = num_hiddens
        # å±‚æ•° = num_layers
        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,
                          dropout=dropout)
        # è¾“å‡ºå±‚ï¼šå°†RNNéšè—çŠ¶æ€æ˜ å°„åˆ°è¯æ±‡è¡¨ç©ºé—´
        self.dense = nn.Linear(num_hiddens, vocab_size)

    # enc_outputs: ç¼–ç å™¨çš„è¾“å‡ºå…ƒç»„ (output, state)
    def init_state(self, enc_outputs, *args):
        """åˆå§‹åŒ–è§£ç å™¨çŠ¶æ€ï¼ˆä½¿ç”¨ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€ï¼‰"""
        # enc_outputs[1] æ˜¯ç¼–ç å™¨RNNçš„æœ€ç»ˆéšè—çŠ¶æ€
        # å½¢çŠ¶ï¼š(num_layers, batch_size, num_hiddens)
        return enc_outputs[1] # è¿”å›:è§£ç å™¨çš„åˆå§‹çŠ¶æ€ï¼ˆç›´æ¥ä½¿ç”¨ç¼–ç å™¨çš„æœ€ç»ˆéšè—çŠ¶æ€ï¼‰

    """
    å‰å‘ä¼ æ’­é€»è¾‘ï¼š
    X: å½“å‰æ—¶é—´æ­¥çš„è¾“å…¥ï¼ˆè§£ç å™¨è¾“å…¥åºåˆ—ï¼‰ï¼Œå½¢çŠ¶ (batch_size, num_steps)
    state: ç¼–ç å™¨æä¾›çš„åˆå§‹éšè—çŠ¶æ€ï¼ˆæˆ–ä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€ï¼‰
    è¿”å›:
    output: é¢„æµ‹è¾“å‡ºï¼Œå½¢çŠ¶ (batch_size, num_steps, vocab_size)
    state: æ›´æ–°åçš„éšè—çŠ¶æ€
    """
    def forward(self, X, state):
        # 1. è¯åµŒå…¥ï¼šå°†è¾“å…¥ç´¢å¼•è½¬æ¢ä¸ºå‘é‡
        # self.embedding(X) å°†è¾“å…¥ç´¢å¼•è½¬æ¢ä¸ºå‘é‡ï¼Œ
        #       å½¢çŠ¶(batch_size,num_steps)â†’(batch_size,num_steps,embed_size)
        # .permute(1, 0, 2) å°†å‰ä¸¤ä¸ªç»´åº¦ä½ç½®äº¤æ¢ï¼Œ(PyTorchçš„RNNè¦æ±‚åºåˆ—ç»´åº¦åœ¨å‰)
        #       å½¢çŠ¶(batch_size,num_steps,embed_size)â†’(num_steps,batch_size,embed_size)
        X = self.embedding(X).permute(1, 0, 2) # å½¢çŠ¶(num_steps,batch_size,embed_size)

        # 2. å‡†å¤‡ä¸Šä¸‹æ–‡å˜é‡ï¼ˆæ¥è‡ªç¼–ç å™¨çš„æœ€ç»ˆéšè—çŠ¶æ€ï¼‰
        # state[-1] å–æœ€åä¸€å±‚çš„éšè—çŠ¶æ€ï¼Œå½¢çŠ¶ï¼š(batch_size, num_hiddens)
        # .repeat(X.shape[0], 1, 1)æ‰©å±•æ—¶é—´æ­¥ç»´åº¦ï¼Œä½¿å…¶ä¸Xçš„num_stepsä¸€è‡´
        # å³ æ²¿ç¬¬ä¸€ç»´é‡å¤ç›´è‡³æ‰©å±•ä¸º ä¸Xçš„ç¬¬0ç»´num_stepsåŒå¤§å°
        # å¹¿æ’­contextï¼Œä½¿å…¶å…·æœ‰ä¸Xç›¸åŒçš„num_steps
        context = state[-1].repeat(X.shape[0], 1, 1) # å½¢çŠ¶ï¼š(num_steps,batch_size,num_hiddens)

        # 3. æ‹¼æ¥å½“å‰è¾“å…¥å’Œä¸Šä¸‹æ–‡å˜é‡ï¼ˆæ²¿ç¬¬3ç»´æ‹¼æ¥ï¼‰
        # X_and_contextå½¢çŠ¶ï¼š(num_steps, batch_size, embed_size + num_hiddens)
        X_and_context = torch.cat((X, context), 2)

        # 4. RNNå¤„ç†ï¼šè¾“å…¥æ‹¼æ¥åçš„å¼ é‡ï¼Œæ›´æ–°éšè—çŠ¶æ€
        # outputå½¢çŠ¶ï¼š(num_steps, batch_size, num_hiddens)
        # stateå½¢çŠ¶ï¼š(num_layers, batch_size, num_hiddens)
        output, state = self.rnn(X_and_context, state)

        # 5. ç”Ÿæˆé¢„æµ‹ï¼šå°†RNNè¾“å‡ºæ˜ å°„åˆ°è¯æ±‡è¡¨ç©ºé—´
        # è°ƒæ•´ç»´åº¦é¡ºåºä¸º (batch_size, num_steps, vocab_size)
        output = self.dense(output).permute(1, 0, 2)
        # outputçš„å½¢çŠ¶:(batch_size,num_steps,vocab_size)  é¢„æµ‹è¾“å‡º
        # stateçš„å½¢çŠ¶:(num_layers,batch_size,num_hiddens) æ›´æ–°åçš„éšè—çŠ¶æ€
        return output, state

# ç›®æ ‡è¯æ±‡å¤§å°ä¸º10ï¼ŒåµŒå…¥ç»´åº¦ä¸º8
# éšè—å±‚ç»´åº¦ä¸º16(ä¸ç¼–ç å™¨éšè—å±‚å¤§å°ç›¸åŒ)ï¼Œéšè—å±‚å †å æ¬¡æ•°ä¸º2(ä¸ç¼–ç å™¨å±‚æ•°ç›¸åŒ)
decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,
                         num_layers=2)
decoder.eval() # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­dropoutç­‰è®­ç»ƒä¸“ç”¨å±‚ï¼‰
state = decoder.init_state(encoder(X)) # åˆå§‹åŒ–è§£ç å™¨çŠ¶æ€
output, state = decoder(X, state) # å‰å‘ä¼ æ’­
output.shape, state.shape
print(f"è§£ç å™¨çš„è¾“å‡º(æ‰€æœ‰æ—¶é—´æ­¥)ï¼š{output.shape}")
print(f"æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„å¤šå±‚éšçŠ¶æ€çš„å½¢çŠ¶ï¼š{state.shape}")


X = torch.tensor([[1, 2, 3], [4, 5, 6]])
print(f"å‡è®¾ä¸¤ä¸ªåºåˆ—çš„æœ‰æ•ˆé•¿åº¦ï¼ˆä¸åŒ…æ‹¬å¡«å……è¯å…ƒï¼‰åˆ†åˆ«ä¸º1å’Œ2 â†“"
      f"\nå±è”½å‰çš„åŸå§‹åºåˆ—ï¼š\n{X}"
      f"\nåœ¨åºåˆ—ä¸­å±è”½ä¸ç›¸å…³çš„é¡¹å çš„åºåˆ—æ•ˆæœï¼š"
      f"\n{common.sequence_mask(X, torch.tensor([1, 2]))}")

X = torch.ones(2, 3, 4)
print(f"å±è”½æœ€åå‡ ä¸ªè½´ä¸Šçš„æ‰€æœ‰é¡¹ï¼Œä¸”ä½¿ç”¨æŒ‡å®šçš„éé›¶å€¼-1æ¥æ›¿æ¢è¿™äº›é¡¹ â†“"
      f"\nå±è”½å‰çš„åŸå§‹åºåˆ—ï¼š\n{X}"
      f"\nåœ¨åºåˆ—ä¸­å±è”½ä¸ç›¸å…³çš„é¡¹å çš„åºåˆ—æ•ˆæœï¼š"
      f"\n{common.sequence_mask(X, torch.tensor([1, 2]), value=-1)}")
""" è¾“å‡ºæ•ˆæœè§£æï¼š
ç¬¬ä¸€ä¸ªåºåˆ— (æœ‰æ•ˆé•¿åº¦1):
  [
    [1,1,1,1],  # ä¿ç•™
    [-1,-1,-1,-1], # æ©ç 
    [-1,-1,-1,-1]  # æ©ç 
  ]
ç¬¬äºŒä¸ªåºåˆ— (æœ‰æ•ˆé•¿åº¦2):
  [
    [1,1,1,1],  # ä¿ç•™
    [1,1,1,1],  # ä¿ç•™
    [-1,-1,-1,-1] # æ©ç 
  ]
"""

loss = common.MaskedSoftmaxCELoss() # åˆ›å»ºæŸå¤±å‡½æ•°å®ä¾‹
# è®¡ç®—æŸå¤±
result = loss(
     torch.ones(3, 4, 10),  # predï¼š3ä¸ªåºåˆ—ï¼Œ4ä¸ªæ—¶é—´æ­¥ï¼Œ10ä¸ªè¯æ±‡è¡¨
     torch.ones((3, 4), dtype=torch.long),  # labelï¼šæ‰€æœ‰æ ‡ç­¾ä¸º1
     torch.tensor([4, 2, 0]))               # valid_lenï¼šæœ‰æ•ˆé•¿åº¦
print(f"è®¡ç®—æŸå¤±ï¼š{result}")


# è¯åµŒå…¥ç»´åº¦32ï¼Œéšè—å±‚ç»´åº¦32ï¼Œrnnå±‚æ•°2ï¼Œéšæœºå¤±æ´»ç‡0.1
embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1
batch_size, num_steps = 64, 10 # æ‰¹é‡å¤§å°å’Œåºåˆ—é•¿åº¦
lr, num_epochs, device = 0.005, 300, common.try_gpu() # å­¦ä¹ ç‡ï¼Œè®­ç»ƒè½®æ•°

# åŠ è½½æ•°æ®ï¼ˆæ³•è¯­â†’è‹±è¯­ç¿»è¯‘ç¤ºä¾‹ï¼‰
# æ•°æ®è¿­ä»£å™¨ï¼Œæºè¯­è¨€çš„è¯è¡¨ï¼Œç›®æ ‡è¯­è¨€çš„è¯è¡¨
train_iter, src_vocab, tgt_vocab = common.load_data_nmt(downloader, batch_size, num_steps)

# å®šä¹‰ç¼–ç å™¨ï¼ˆåŒå‘GRUï¼‰å’Œè§£ç å™¨ï¼ˆå•å‘GRUï¼‰
encoder = Seq2SeqEncoder(len(src_vocab), # æºè¯­è¨€è¯è¡¨å¤§å°
                        embed_size, num_hiddens, num_layers,dropout)
decoder = Seq2SeqDecoder(len(tgt_vocab), # ç›®æ ‡è¯­è¨€è¯è¡¨å¤§å°
                        embed_size, num_hiddens, num_layers,dropout)
net = common.EncoderDecoder(encoder, decoder) # ç»„åˆä¸ºSeq2Seqæ¨¡å‹
common.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device) # å¯åŠ¨è®­ç»ƒ

# è‹±è¯­å¥å­åˆ—è¡¨ï¼ˆæºè¯­è¨€ï¼‰
engs = ['go .', "i lost .", 'he\'s calm .', 'i\'m home .']
# å¯¹åº”çš„æ³•è¯­å¥å­åˆ—è¡¨ï¼ˆç›®æ ‡è¯­è¨€/å‚è€ƒè¯‘æ–‡ï¼‰
fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']
for eng, fra in zip(engs, fras): # éå†æ¯ä¸ªè‹±è¯­-æ³•è¯­å¥å­å¯¹
    # ä½¿ç”¨seq2seqæ¨¡å‹è¿›è¡Œç¿»è¯‘é¢„æµ‹
    translation, attention_weight_seq = common.predict_seq2seq(
        net, eng, src_vocab, tgt_vocab, num_steps, device)
    # è®¡ç®—å¹¶æ‰“å°BLEUåˆ†æ•°ï¼ˆä½¿ç”¨1-gramå’Œ2-gramï¼‰
    bleu_score = common.bleu(translation, fra, k=2)
    # è¾“å‡ºç»“æœï¼šåŸå¥ => é¢„æµ‹ç¿»è¯‘, BLEUåˆ†æ•°
    print(f'æº({eng}) => é¢„æµ‹({translation}), ç›¸ä¼¼åº¦è¯„ä¼°bleu {bleu_score:.3f}')




