import torch
from torch import nn
import matplotlib.pyplot as plt
import numpy as np
import common

# print(f"10x10 çš„å•ä½çŸ©é˜µï¼ˆå¯¹è§’çº¿ä¸º 1ï¼Œå…¶ä½™ä¸º 0ï¼‰ï¼š\n{torch.eye(10)}")

# ä»…å½“æŸ¥è¯¢å’Œé”®ç›¸åŒæ—¶ï¼Œæ³¨æ„åŠ›æƒé‡ä¸º1ï¼Œå¦åˆ™ä¸º0
# torch.eye(10): ç”Ÿæˆä¸€ä¸ª 10x10 çš„å•ä½çŸ©é˜µï¼ˆå¯¹è§’çº¿ä¸º 1ï¼Œå…¶ä½™ä¸º 0ï¼‰
# .reshape((1,1,10,10)): è°ƒæ•´å½¢çŠ¶ä¸º(batch_size=1,num_heads=1,seq_len=10,seq_len=10)
# æ¨¡æ‹Ÿä¸€ä¸ªå•å¤´æ³¨æ„åŠ›æœºåˆ¶çš„æƒé‡çŸ©é˜µï¼ˆQueries Ã— Keysï¼‰
attention_weights = torch.eye(10).reshape((1, 1, 10, 10))
common.show_heatmaps(attention_weights, xlabel='Keys', ylabel='Queries')



n_train = 50  # è®­ç»ƒæ ·æœ¬æ•°
x_train, _ = torch.sort(torch.rand(n_train) * 5) # æ’åºåçš„è®­ç»ƒæ ·æœ¬(ä»¥ä¾¿å¯è§†åŒ–)

def f(x): # éçº¿æ€§å‡½æ•°
    return 2 * torch.sin(x) + x**0.8

y_train = f(x_train) + torch.normal(0.0, 0.5, (n_train,))  # è®­ç»ƒæ ·æœ¬çš„è¾“å‡º(æ·»åŠ å™ªå£°é¡¹)
x_test = torch.arange(0, 5, 0.1) # æµ‹è¯•æ ·æœ¬
y_truth = f(x_test)              # æµ‹è¯•æ ·æœ¬çš„çœŸå®è¾“å‡º
n_test = len(x_test)             # æµ‹è¯•æ ·æœ¬æ•°
print(f"æµ‹è¯•æ ·æœ¬æ•°ï¼š{n_test}")

def plot_kernel_reg(y_hat):
    common.plot_kernel_reg(x_test, y_truth, y_hat, x_train, y_train)

def average_aggregation():
    '''ğŸ‘‰ å¹³å‡æ±‡èš'''
    # .repeat_interleave() ç”¨äºé‡å¤å¼ é‡å…ƒç´ 
    # è®¡ç®—å‡ºè®­ç»ƒæ ·æœ¬æ ‡ç­¾yçš„å‡å€¼ï¼Œç„¶åå°†å…¶é‡å¤n_testæ¬¡ï¼Œè¿”å›ä¸€ä¸ªé•¿åº¦ä¸ºn_testçš„ 1Då¼ é‡
    # å³ï¼Œç”Ÿæˆä¸€ä¸ªé•¿åº¦ä¸º n_test çš„å¼ é‡ï¼Œæ‰€æœ‰å…ƒç´ ä¸º y_trainçš„å‡å€¼ï¼Œå³ y_train.mean()
    y_hat = torch.repeat_interleave(y_train.mean(), n_test)
    # plot_kernel_reg(y_hat)
    common.plot_kernel_reg(x_test, y_truth, y_hat, x_train, y_train)
# average_aggregation()

def nonParametric_attention_aggregation():
    '''ğŸ‘‰ éå‚æ•°æ³¨æ„åŠ›æ±‡èš'''
    # 1. é‡å¤x_testä»¥åŒ¹é…æ³¨æ„åŠ›æƒé‡çš„å½¢çŠ¶
    # ä¸ºæ¯ä¸ªæµ‹è¯•æ ·æœ¬ç”Ÿæˆä¸€ä¸ªä¸ x_train å¯¹é½çš„çŸ©é˜µï¼Œä¾¿äºåç»­è®¡ç®—ç›¸ä¼¼åº¦
    # X_repeatçš„å½¢çŠ¶:(n_test,n_train),
    # æ¯ä¸€è¡Œéƒ½åŒ…å«ç€ç›¸åŒçš„æµ‹è¯•è¾“å…¥ï¼ˆä¾‹å¦‚ï¼šåŒæ ·çš„æŸ¥è¯¢ï¼‰
    # x_test.repeat_interleave(n_train) å¯¹å¼ é‡x_testçš„æ¯ä¸ªå…ƒç´ æ²¿æŒ‡å®šç»´åº¦(é»˜è®¤0)é‡å¤n_trainæ¬¡
    # å³ æ¯ä¸ªæµ‹è¯•æ ·æœ¬é‡å¤ n_train æ¬¡ï¼ˆå±•å¹³ä¸ºä¸€ç»´ï¼‰
    # .reshape((-1, n_train)) å°†å±•å¹³çš„å¼ é‡é‡æ–°è°ƒæ•´ä¸º(n_test, n_train)ï¼Œå…¶ä¸­æ¯ä¸€è¡Œæ˜¯x_testçš„ä¸€ä¸ªå‰¯æœ¬ é‡å¤n_trainæ¬¡
    X_repeat = x_test.repeat_interleave(n_train).reshape((-1, n_train))
    print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶x_test.shapeï¼š{x_test.shape}")
    print(f"å°†æµ‹è¯•æ•°æ®å…ƒç´ é‡å¤åå½¢çŠ¶ x_test.repeat_interleave(n_train).shapeï¼š"
          f"{x_test.repeat_interleave(n_train).shape}")
    print(f"é‡å¡‘å½¢çŠ¶ä¸º(n_test,n_train)ä»¥ä¾¿åç»­è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç›¸ä¼¼åº¦è¶Šé«˜ï¼Œæ³¨æ„åŠ›æƒé‡è¶Šå¤§ï¼‰\n"
          f"x_test.repeat_interleave(n_train).reshape((-1, n_train)).shapeï¼š"
          f"{x_test.repeat_interleave(n_train).reshape((-1, n_train)).shape}")

    # 2. è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼ˆé«˜æ–¯æ ¸è½¯æ³¨æ„åŠ›ï¼‰
    # x_trainåŒ…å«ç€é”®ã€‚attention_weightsçš„å½¢çŠ¶ï¼š(n_test,n_train),
    # æ¯ä¸€è¡Œéƒ½åŒ…å«ç€è¦åœ¨ç»™å®šçš„æ¯ä¸ªæŸ¥è¯¢çš„å€¼ï¼ˆy_trainï¼‰ä¹‹é—´åˆ†é…çš„æ³¨æ„åŠ›æƒé‡
    # -(X_repeat - x_train)**2 / 2 è®¡ç®—æµ‹è¯•æ ·æœ¬ä¸è®­ç»ƒæ ·æœ¬ä¹‹é—´çš„è´Ÿæ¬§æ°è·ç¦»ï¼ˆé«˜æ–¯æ ¸çš„æŒ‡æ•°éƒ¨åˆ†ï¼‰
    # æµ‹è¯•æ•°æ®çš„è¾“å…¥X_repeat ç›¸å½“äº æŸ¥è¯¢
    # è®­ç»ƒæ•°æ®çš„è¾“å…¥x_train  ç›¸å½“äº é”®
    attention_weights = nn.functional.softmax(
        -(X_repeat - x_train)**2 / 2, # é«˜æ–¯æ ¸(è´Ÿæ¬§æ°è·ç¦»)ï¼Œç›¸ä¼¼åº¦è®¡ç®—ï¼šè·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ï¼ˆæƒé‡è¶Šå¤§ï¼‰
        dim=1) # å¯¹æ¯ä¸€è¡Œï¼ˆæ¯ä¸ªæµ‹è¯•æ ·æœ¬ï¼‰åšsoftmaxå½’ä¸€åŒ–ï¼Œç¡®ä¿æƒé‡å’Œä¸º1

    # 3. åŠ æƒå¹³å‡å¾—åˆ°é¢„æµ‹å€¼
    # y_hatçš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯å€¼çš„åŠ æƒå¹³å‡å€¼ï¼Œå…¶ä¸­çš„æƒé‡æ˜¯æ³¨æ„åŠ›æƒé‡
    # å·¦ï¼šæ¯ä¸ªæµ‹è¯•æ ·æœ¬ å¯¹åº”æ‰€æœ‰è®­ç»ƒæ ‡ç­¾çš„å„ä¸ªæ³¨æ„åŠ›æƒé‡
    # å³ï¼šæ¯ä¸ªè®­ç»ƒæ ·æœ¬å¯¹åº”çš„æ ‡ç­¾
    y_hat = torch.matmul(attention_weights, y_train) # çŸ©é˜µä¹˜æ³•ï¼šå·¦æ‰¾è¡Œï¼Œå³æ‰¾åˆ—
    plot_kernel_reg(y_hat)

    # np.expand_dims(attention_weights, 0) åœ¨ç¬¬0è½´(æœ€å¤–å±‚)æ‰©å±•æ–°ç»´åº¦
    # np.expand_dims(np.expand_dims(attention_weights,0),0) è¿ç»­ä¸¤æ¬¡åœ¨ç¬¬0è½´(æœ€å¤–å±‚)æ‰©å±•æ–°ç»´åº¦
    # å‡è®¾attention_weightsåŸå§‹ç»´åº¦æ˜¯(3,4)ï¼Œåˆ™ç¬¬ä¸€æ¬¡æ‰©å±•å˜æˆ(1,3,4)ï¼Œåˆ™ç¬¬ä¸€æ¬¡æ‰©å±•å˜æˆ(1,1,3,4)
    common.show_heatmaps(np.expand_dims(np.expand_dims(attention_weights, 0), 0),
                      xlabel='Sorted training inputs',
                      ylabel='Sorted testing inputs') # æ˜¾ç¤ºæ³¨æ„åŠ›æƒé‡çš„çŸ©é˜µçƒ­å›¾
# nonParametric_attention_aggregation()

# def parametric_attention_aggregation():
# ğŸ‘‰ å‚æ•°æ³¨æ„åŠ›æ±‡èš

















