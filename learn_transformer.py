import torch
from torch import nn
import matplotlib.pyplot as plt
import numpy as np
import common

# print(f"10x10 çš„å•ä½çŸ©é˜µï¼ˆå¯¹è§’çº¿ä¸º 1ï¼Œå…¶ä½™ä¸º 0ï¼‰ï¼š\n{torch.eye(10)}")

# ä»…å½“æŸ¥è¯¢å’Œé”®ç›¸åŒæ—¶ï¼Œæ³¨æ„åŠ›æƒé‡ä¸º1ï¼Œå¦åˆ™ä¸º0
# torch.eye(10): ç”Ÿæˆä¸€ä¸ª 10x10 çš„å•ä½çŸ©é˜µï¼ˆå¯¹è§’çº¿ä¸º 1ï¼Œå…¶ä½™ä¸º 0ï¼‰
# .reshape((1,1,10,10)): è°ƒæ•´å½¢çŠ¶ä¸º(batch_size=1,num_heads=1,seq_len=10,seq_len=10)
# æ¨¡æ‹Ÿä¸€ä¸ªå•å¤´æ³¨æ„åŠ›æœºåˆ¶çš„æƒé‡çŸ©é˜µï¼ˆQueries Ã— Keysï¼‰
attention_weights = torch.eye(10).reshape((1, 1, 10, 10))
# common.show_heatmaps(attention_weights, xlabel='Keys', ylabel='Queries')



n_train = 50  # è®­ç»ƒæ ·æœ¬æ•°
x_train, _ = torch.sort(torch.rand(n_train) * 5) # æ’åºåçš„è®­ç»ƒæ ·æœ¬(ä»¥ä¾¿å¯è§†åŒ–)

def f(x): # éçº¿æ€§å‡½æ•°
    return 2 * torch.sin(x) + x**0.8

y_train = f(x_train) + torch.normal(0.0, 0.5, (n_train,))  # è®­ç»ƒæ ·æœ¬çš„è¾“å‡º(æ·»åŠ å™ªå£°é¡¹)
x_test = torch.arange(0, 5, 0.1) # æµ‹è¯•æ ·æœ¬
y_truth = f(x_test)              # æµ‹è¯•æ ·æœ¬çš„çœŸå®è¾“å‡º
n_test = len(x_test)             # æµ‹è¯•æ ·æœ¬æ•°
print(f"æµ‹è¯•æ ·æœ¬æ•°ï¼š{n_test}")

def plot_kernel_reg(y_hat):
    common.plot_kernel_reg(x_test, y_truth, y_hat, x_train, y_train)

def average_aggregation():
    '''ğŸ‘‰ å¹³å‡æ±‡èš'''
    # .repeat_interleave() ç”¨äºé‡å¤å¼ é‡å…ƒç´ 
    # è®¡ç®—å‡ºè®­ç»ƒæ ·æœ¬æ ‡ç­¾yçš„å‡å€¼ï¼Œç„¶åå°†å…¶é‡å¤n_testæ¬¡ï¼Œè¿”å›ä¸€ä¸ªé•¿åº¦ä¸ºn_testçš„ 1Då¼ é‡
    # å³ï¼Œç”Ÿæˆä¸€ä¸ªé•¿åº¦ä¸º n_test çš„å¼ é‡ï¼Œæ‰€æœ‰å…ƒç´ ä¸º y_trainçš„å‡å€¼ï¼Œå³ y_train.mean()
    y_hat = torch.repeat_interleave(y_train.mean(), n_test)
    # plot_kernel_reg(y_hat)
    common.plot_kernel_reg(x_test, y_truth, y_hat, x_train, y_train)
# average_aggregation()

def nonParametric_attention_aggregation():
    '''ğŸ‘‰ éå‚æ•°æ³¨æ„åŠ›æ±‡èš'''
    # 1. é‡å¤x_testä»¥åŒ¹é…æ³¨æ„åŠ›æƒé‡çš„å½¢çŠ¶
    # ä¸ºæ¯ä¸ªæµ‹è¯•æ ·æœ¬ç”Ÿæˆä¸€ä¸ªä¸ x_train å¯¹é½çš„çŸ©é˜µï¼Œä¾¿äºåç»­è®¡ç®—ç›¸ä¼¼åº¦
    # X_repeatçš„å½¢çŠ¶:(n_test,n_train),
    # æ¯ä¸€è¡Œéƒ½åŒ…å«ç€ç›¸åŒçš„æµ‹è¯•è¾“å…¥ï¼ˆä¾‹å¦‚ï¼šåŒæ ·çš„æŸ¥è¯¢ï¼‰
    # x_test.repeat_interleave(n_train) å¯¹å¼ é‡x_testçš„æ¯ä¸ªå…ƒç´ æ²¿æŒ‡å®šç»´åº¦(é»˜è®¤0)é‡å¤n_trainæ¬¡
    # å³ æ¯ä¸ªæµ‹è¯•æ ·æœ¬é‡å¤ n_train æ¬¡ï¼ˆå±•å¹³ä¸ºä¸€ç»´ï¼‰
    # .reshape((-1, n_train)) å°†å±•å¹³çš„å¼ é‡é‡æ–°è°ƒæ•´ä¸º(n_test, n_train)ï¼Œå…¶ä¸­æ¯ä¸€è¡Œæ˜¯x_testçš„ä¸€ä¸ªå‰¯æœ¬ é‡å¤n_trainæ¬¡
    X_repeat = x_test.repeat_interleave(n_train).reshape((-1, n_train))
    print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶x_test.shapeï¼š{x_test.shape}")
    print(f"å°†æµ‹è¯•æ•°æ®å…ƒç´ é‡å¤åå½¢çŠ¶ x_test.repeat_interleave(n_train).shapeï¼š"
          f"{x_test.repeat_interleave(n_train).shape}")
    print(f"é‡å¡‘å½¢çŠ¶ä¸º(n_test,n_train)ä»¥ä¾¿åç»­è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç›¸ä¼¼åº¦è¶Šé«˜ï¼Œæ³¨æ„åŠ›æƒé‡è¶Šå¤§ï¼‰\n"
          f"x_test.repeat_interleave(n_train).reshape((-1, n_train)).shapeï¼š"
          f"{x_test.repeat_interleave(n_train).reshape((-1, n_train)).shape}")

    # 2. è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼ˆé«˜æ–¯æ ¸è½¯æ³¨æ„åŠ›ï¼‰
    # x_trainåŒ…å«ç€é”®ã€‚attention_weightsçš„å½¢çŠ¶ï¼š(n_test,n_train),
    # æ¯ä¸€è¡Œéƒ½åŒ…å«ç€è¦åœ¨ç»™å®šçš„æ¯ä¸ªæŸ¥è¯¢çš„å€¼ï¼ˆy_trainï¼‰ä¹‹é—´åˆ†é…çš„æ³¨æ„åŠ›æƒé‡
    # -(X_repeat - x_train)**2 / 2 è®¡ç®—æµ‹è¯•æ ·æœ¬ä¸è®­ç»ƒæ ·æœ¬ä¹‹é—´çš„è´Ÿæ¬§æ°è·ç¦»ï¼ˆé«˜æ–¯æ ¸çš„æŒ‡æ•°éƒ¨åˆ†ï¼‰
    # æµ‹è¯•æ•°æ®çš„è¾“å…¥X_repeat ç›¸å½“äº æŸ¥è¯¢
    # è®­ç»ƒæ•°æ®çš„è¾“å…¥x_train  ç›¸å½“äº é”®
    attention_weights = nn.functional.softmax(
        -(X_repeat - x_train)**2 / 2, # é«˜æ–¯æ ¸(è´Ÿæ¬§æ°è·ç¦»)ï¼Œç›¸ä¼¼åº¦è®¡ç®—ï¼šè·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ï¼ˆæƒé‡è¶Šå¤§ï¼‰
        dim=1) # å¯¹æ¯ä¸€è¡Œï¼ˆæ¯ä¸ªæµ‹è¯•æ ·æœ¬ï¼‰åšsoftmaxå½’ä¸€åŒ–ï¼Œç¡®ä¿æƒé‡å’Œä¸º1

    # 3. åŠ æƒå¹³å‡å¾—åˆ°é¢„æµ‹å€¼
    # y_hatçš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯å€¼çš„åŠ æƒå¹³å‡å€¼ï¼Œå…¶ä¸­çš„æƒé‡æ˜¯æ³¨æ„åŠ›æƒé‡
    # å·¦ï¼šæ¯ä¸ªæµ‹è¯•æ ·æœ¬ å¯¹åº”æ‰€æœ‰è®­ç»ƒæ ‡ç­¾çš„å„ä¸ªæ³¨æ„åŠ›æƒé‡
    # å³ï¼šæ¯ä¸ªè®­ç»ƒæ ·æœ¬å¯¹åº”çš„æ ‡ç­¾
    y_hat = torch.matmul(attention_weights, y_train) # çŸ©é˜µä¹˜æ³•ï¼šå·¦æ‰¾è¡Œï¼Œå³æ‰¾åˆ—
    plot_kernel_reg(y_hat)

    # np.expand_dims(attention_weights, 0) åœ¨ç¬¬0è½´(æœ€å¤–å±‚)æ‰©å±•æ–°ç»´åº¦
    # np.expand_dims(np.expand_dims(attention_weights,0),0) è¿ç»­ä¸¤æ¬¡åœ¨ç¬¬0è½´(æœ€å¤–å±‚)æ‰©å±•æ–°ç»´åº¦
    # å‡è®¾attention_weightsåŸå§‹ç»´åº¦æ˜¯(3,4)ï¼Œåˆ™ç¬¬ä¸€æ¬¡æ‰©å±•å˜æˆ(1,3,4)ï¼Œåˆ™ç¬¬ä¸€æ¬¡æ‰©å±•å˜æˆ(1,1,3,4)
    common.show_heatmaps(np.expand_dims(np.expand_dims(attention_weights, 0), 0),
                      xlabel='Sorted training inputs',
                      ylabel='Sorted testing inputs') # æ˜¾ç¤ºæ³¨æ„åŠ›æƒé‡çš„çŸ©é˜µçƒ­å›¾
# nonParametric_attention_aggregation()

# def parametric_attention_aggregation():
# ğŸ‘‰ å¸¦å‚æ•°æ³¨æ„åŠ›æ±‡èš
X = torch.ones((2, 1, 4))
Y = torch.ones((2, 4, 6))
print(f"æ‰¹é‡çŸ©é˜µä¹˜æ³•bmmåï¼Œç»“æœçŸ©é˜µå½¢çŠ¶ï¼š{torch.bmm(X, Y).shape}")

# æ¼”ç¤ºæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„åŠ æƒæ±‚å’Œ
weights = torch.ones((2, 10)) * 0.1          # å½¢çŠ¶: (æ‰¹é‡å¤§å°, åºåˆ—é•¿åº¦)-æ³¨æ„åŠ›æƒé‡
values = torch.arange(20.0).reshape((2, 10)) # å½¢çŠ¶: (æ‰¹é‡å¤§å°, åºåˆ—é•¿åº¦)-å€¼å‘é‡
# .unsqueeze()åœ¨æŒ‡å®šä½ç½®å¢åŠ ç»´åº¦
print(f"åœ¨æŒ‡å®šä½ç½®å¢åŠ ç»´åº¦åçŸ©é˜µå½¢çŠ¶ï¼š\n"
      f"weightsï¼š{weights.unsqueeze(1).shape}\n"
      f"values ï¼š{values.unsqueeze(-1).shape}")
print(f"å¢åŠ ç»´åº¦å è¿›è¡Œ æ‰¹é‡çŸ©é˜µä¹˜æ³•bmmï¼Œç»“æœä¸ºï¼š\n"
      f"ä½¿ç”¨bmmè®¡ç®—åŠ æƒå’Œ: æƒé‡(2,1,10) Ã— å€¼(2,10,1) = ç»“æœ(2,1,1) \n"
      f"{torch.bmm(weights.unsqueeze(1), values.unsqueeze(-1))}")


class NWKernelRegression(nn.Module):
    ''' Nadaraya-Watson æ ¸å›å½’æ¨¡å‹,å®ç°åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å›å½’ '''
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # å¯å­¦ä¹ çš„å‚æ•° (é«˜æ–¯æ ¸çš„å¸¦å®½å‚æ•°)ï¼Œå³ æŸ¥è¯¢ä¸é”®é—´è·ç¦»è¦ä¹˜ä»¥çš„æƒé‡
        self.w = nn.Parameter(torch.rand((1,), requires_grad=True))

    def forward(self, queries, keys, values):
        '''
        queries : æŸ¥è¯¢è¾“å…¥ (n_query, dim)
        keys    : è®­ç»ƒè¾“å…¥ (n_train, dim)
        values  : è®­ç»ƒè¾“å‡º (n_train, dim)
        # queries å’Œ attention_weightsçš„å½¢çŠ¶ä¸º (æŸ¥è¯¢ä¸ªæ•°ï¼Œâ€œé”®ï¼å€¼â€å¯¹ä¸ªæ•°)
        '''
        # æ‰©å±• æŸ¥è¯¢å‘é‡querieså½¢çŠ¶ ä»¥åŒ¹é… é”®å€¼å¯¹keysçš„ç»´åº¦
        # querieså½¢çŠ¶: (æŸ¥è¯¢ä¸ªæ•°,) -> æ‰©å±•ä¸º (æŸ¥è¯¢ä¸ªæ•°, é”®å€¼å¯¹ä¸ªæ•°)
        queries = queries.repeat_interleave(keys.shape[1]).reshape((-1, keys.shape[1]))
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼ˆä½¿ç”¨é«˜æ–¯æ ¸å‡½æ•°ï¼‰
        # å…¬å¼: attention = softmax(-(query - key)^2 * w^2 / 2)
        # æ³¨æ„åŠ›æƒé‡é€šè¿‡é«˜æ–¯æ ¸ exp(-(x_query-x_key)^2 / (2Ïƒ^2)) è®¡ç®—ï¼Œè¿™é‡Œç”¨softmaxå½’ä¸€åŒ–
        self.attention_weights = nn.functional.softmax(
            -((queries - keys) * self.w) ** 2 / 2, dim=1) # å½¢çŠ¶ (n_query, n_train)
        # ä½¿ç”¨æ³¨æ„åŠ›æƒé‡å¯¹å€¼è¿›è¡ŒåŠ æƒæ±‚å’Œå¾—åˆ°é¢„æµ‹å€¼
        # bmm: (æ‰¹é‡å¤§å°, 1, é”®å€¼å¯¹ä¸ªæ•°) Ã— (æ‰¹é‡å¤§å°, é”®å€¼å¯¹ä¸ªæ•°, 1) = (æ‰¹é‡å¤§å°, 1, 1)
        # valuesçš„å½¢çŠ¶ä¸º(æŸ¥è¯¢ä¸ªæ•°ï¼Œâ€œé”®ï¼å€¼â€å¯¹ä¸ªæ•°)
        return torch.bmm(self.attention_weights.unsqueeze(1), # (n_query, 1, n_train)
                         values.unsqueeze(-1)).reshape(-1) # (n_query, 1, 1) â†’ (n_query,)

# å‡†å¤‡è®­ç»ƒæ—¶çš„ keys å’Œ values
# ç”Ÿæˆè®­ç»ƒæ•°æ®çš„æ‰€æœ‰ç»„åˆï¼ˆç”¨äºè‡ªæ³¨æ„åŠ›ï¼‰
# X_tileçš„å½¢çŠ¶:(n_trainï¼Œn_train)ï¼Œæ¯ä¸€è¡Œéƒ½åŒ…å«ç€ç›¸åŒçš„è®­ç»ƒè¾“å…¥
# Y_tileçš„å½¢çŠ¶:(n_trainï¼Œn_train)ï¼Œæ¯ä¸€è¡Œéƒ½åŒ…å«ç€ç›¸åŒçš„è®­ç»ƒè¾“å‡º
X_tile = x_train.repeat((n_train, 1)) # å½¢çŠ¶ (n_train * n_train, dim)
Y_tile = y_train.repeat((n_train, 1)) # å½¢çŠ¶ (n_train * n_train, dim)

# åˆ›å»ºé”®å’Œå€¼ï¼ˆæ’é™¤å¯¹è§’çº¿å…ƒç´ ï¼Œå³è‡ªèº«  é¿å…è‡ªåŒ¹é…ï¼‰
# mask ç”¨äºæ’é™¤è‡ªåŒ¹é…ï¼ˆå³æŸ¥è¯¢ç‚¹ä¸ä¸è‡ªèº«è®¡ç®—æ³¨æ„åŠ›ï¼‰
mask = (1 - torch.eye(n_train)).type(torch.bool) # å½¢çŠ¶ (n_train, n_train)
# keysçš„å½¢çŠ¶  :('n_train'ï¼Œ'n_train'-1)
# valuesçš„å½¢çŠ¶:('n_train'ï¼Œ'n_train'-1)
keys   = X_tile[mask].reshape((n_train, -1)) # å½¢çŠ¶ (n_train, n_train-1)
values = Y_tile[mask].reshape((n_train, -1)) # å½¢çŠ¶ (n_train, n_train-1)


# åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
net = NWKernelRegression()
loss = nn.MSELoss(reduction='none') # é€å…ƒç´ è®¡ç®—æŸå¤±
trainer = torch.optim.SGD(net.parameters(), lr=0.5)
animator = common.Animator(xlabel='epoch', ylabel='loss', xlim=[1, 5])

for epoch in range(5):
    trainer.zero_grad() # æ¸…é›¶æ¢¯åº¦
    # è®¡ç®—æ¯ä¸ªè®­ç»ƒç‚¹çš„é¢„æµ‹å€¼ï¼ˆqueries=x_train, keys/valuesæ¥è‡ªå…¶ä»–ç‚¹ï¼‰
    l = loss(net(x_train, keys, values), y_train) # å‰å‘ä¼ æ’­
    l.sum().backward()  # åå‘ä¼ æ’­ï¼ˆå¯¹æŸå¤±æ±‚å’Œååå‘ä¼ æ’­ï¼‰
    trainer.step()      # æ›´æ–°å‚æ•°
    print(f'epoch {epoch + 1}, loss {float(l.sum()):.6f}')
    animator.add(epoch + 1, float(l.sum()))


# æµ‹è¯•é˜¶æ®µï¼šæ¯ä¸ªæµ‹è¯•ç‚¹ä¸æ‰€æœ‰è®­ç»ƒç‚¹è®¡ç®—æ³¨æ„åŠ›
# keysçš„å½¢çŠ¶ :(n_testï¼Œn_train)ï¼Œæ¯ä¸€è¡ŒåŒ…å«ç€ç›¸åŒçš„è®­ç»ƒè¾“å…¥ï¼ˆä¾‹å¦‚ï¼Œç›¸åŒçš„é”®ï¼‰
# valueçš„å½¢çŠ¶:(n_testï¼Œn_train)
keys   = x_train.repeat((n_test, 1)) # å½¢çŠ¶ (n_test, n_train)
values = y_train.repeat((n_test, 1)) # å½¢çŠ¶ (n_test, n_train)
y_hat = net(x_test, keys, values).unsqueeze(1).detach() # é¢„æµ‹
plot_kernel_reg(y_hat) # ç»˜åˆ¶å›å½’ç»“æœ

# å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡ï¼ˆæµ‹è¯•ç‚¹ vs è®­ç»ƒç‚¹ï¼‰
# net.attention_weightså½¢çŠ¶: (n_test, n_train)
# æ·»åŠ ä¸¤ä¸ªç»´åº¦ä½¿å…¶å˜ä¸º(1, 1, n_test, n_train)ä»¥åŒ¹é…show_heatmapsæœŸæœ›çš„4Dè¾“å…¥
common.show_heatmaps(
    net.attention_weights.unsqueeze(0).unsqueeze(0), # å¢åŠ æ‰¹æ¬¡å’Œå¤´ç»´åº¦
    xlabel='Sorted training inputs',
    ylabel='Sorted testing inputs')
plt.pause(4444)  # é—´éš”çš„ç§’æ•°ï¼š 4s




